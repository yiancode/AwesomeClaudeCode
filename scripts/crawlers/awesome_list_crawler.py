#!/usr/bin/env python3
"""
Awesome List çˆ¬è™« / Awesome List Crawler

ä» GitHub Awesome åˆ—è¡¨ä¸­å‘ç°ä¸ Claude Code ç›¸å…³çš„èµ„æºã€‚
Discovers Claude Code related resources from GitHub Awesome lists.
"""

import os
import re
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

from .base_crawler import BaseCrawler


class AwesomeListCrawler(BaseCrawler):
    """Awesome List çˆ¬è™« / Awesome List crawler"""

    @property
    def name(self) -> str:
        return "Awesome Lists"

    @property
    def source_type(self) -> str:
        return "awesome-list"

    def __init__(self, config: dict, rate_limit_config: Optional[dict] = None):
        super().__init__(config, rate_limit_config)

        # Awesome List ç‰¹å®šé…ç½®
        self.awesome_config = config.get("awesome_lists", {})
        self.lists = self.awesome_config.get("lists", [])
        self.deep_parse = self.awesome_config.get("deep_parse", True)
        self.max_links_per_list = self.awesome_config.get("max_links_per_list", 100)

        # GitHub token
        self.github_token = os.environ.get("GITHUB_TOKEN")
        if self.github_token:
            self.session.headers["Authorization"] = f"Bearer {self.github_token}"

    def _extract_github_repo(self, url: str) -> Optional[Tuple[str, str]]:
        """
        ä» URL æå– GitHub owner/repo / Extract GitHub owner/repo from URL

        Returns: (owner, repo) or None
        """
        if "github.com" not in url:
            return None

        parts = url.rstrip("/").split("/")
        try:
            github_index = next(i for i, p in enumerate(parts) if "github.com" in p)
            if len(parts) > github_index + 2:
                owner = parts[github_index + 1]
                repo = parts[github_index + 2].replace(".git", "")
                return (owner, repo)
        except (StopIteration, IndexError):
            pass

        return None

    def _get_readme_content(self, owner: str, repo: str) -> Optional[str]:
        """
        è·å–ä»“åº“ README å†…å®¹ / Get repository README content

        Args:
            owner: ä»“åº“æ‰€æœ‰è€… / Repository owner
            repo: ä»“åº“å / Repository name

        Returns:
            README å†…å®¹ / README content
        """
        # å°è¯•ä¸åŒçš„ README æ–‡ä»¶å
        readme_names = ["README.md", "readme.md", "Readme.md", "README", "readme"]

        for readme_name in readme_names:
            url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{readme_name}"
            response = self._make_request(url)

            if response:
                return response.text

            # å°è¯• master åˆ†æ”¯
            url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{readme_name}"
            response = self._make_request(url)

            if response:
                return response.text

        return None

    def _parse_markdown_links(self, content: str) -> List[Tuple[str, str, str]]:
        """
        è§£æ Markdown ä¸­çš„é“¾æ¥ / Parse links from Markdown

        Args:
            content: Markdown å†…å®¹ / Markdown content

        Returns:
            é“¾æ¥åˆ—è¡¨ [(title, url, description), ...] / List of links
        """
        links = []

        # åŒ¹é… Markdown é“¾æ¥: [title](url) - description
        # æˆ–è€…: - [title](url) - description
        pattern = r"[-*]?\s*\[([^\]]+)\]\(([^)]+)\)(?:\s*[-â€“â€”]\s*(.+?))?(?=\n|$)"
        matches = re.findall(pattern, content, re.MULTILINE)

        for match in matches:
            title = match[0].strip()
            url = match[1].strip()
            description = match[2].strip() if len(match) > 2 else ""

            # è¿‡æ»¤æ— æ•ˆé“¾æ¥
            if not url or url.startswith("#") or url.startswith("mailto:"):
                continue

            # è§„èŒƒåŒ– URL
            if url.startswith("//"):
                url = "https:" + url
            elif not url.startswith("http"):
                continue

            links.append((title, url, description))

        return links

    def _filter_link(self, title: str, url: str, description: str, keywords: List[str]) -> bool:
        """
        è¿‡æ»¤é“¾æ¥ / Filter link

        Args:
            title: æ ‡é¢˜ / Title
            url: URL
            description: æè¿° / Description
            keywords: å…³é”®è¯åˆ—è¡¨ / Keyword list

        Returns:
            æ˜¯å¦é€šè¿‡è¿‡æ»¤ / Whether passed filter
        """
        # æ’é™¤éç›¸å…³é“¾æ¥
        if not self._is_relevant_url(url):
            return False

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_duplicate(url):
            return False

        # å¦‚æœæœ‰å…³é”®è¯é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
        if keywords:
            combined = f"{title} {description}".lower()
            has_keyword = any(kw.lower() in combined for kw in keywords)
            if not has_keyword:
                return False

        return True

    def _get_repo_info(self, owner: str, repo: str) -> Optional[dict]:
        """
        è·å–ä»“åº“ä¿¡æ¯ / Get repository info

        Args:
            owner: ä»“åº“æ‰€æœ‰è€… / Repository owner
            repo: ä»“åº“å / Repository name

        Returns:
            ä»“åº“ä¿¡æ¯ / Repository info
        """
        url = f"https://api.github.com/repos/{owner}/{repo}"

        headers = {"Accept": "application/vnd.github+json"}
        if self.github_token:
            headers["Authorization"] = f"Bearer {self.github_token}"

        response = self._make_request(url, headers=headers)

        if response:
            return response.json()

        return None

    def _create_resource_from_link(self, title: str, url: str, description: str, source_list: str) -> Optional[dict]:
        """
        ä»é“¾æ¥åˆ›å»ºèµ„æº / Create resource from link

        Args:
            title: æ ‡é¢˜ / Title
            url: URL
            description: æè¿° / Description
            source_list: æ¥æºåˆ—è¡¨åç§° / Source list name

        Returns:
            å€™é€‰èµ„æºæˆ– None / Candidate resource or None
        """
        # å¦‚æœæ˜¯ GitHub é“¾æ¥ï¼Œå°è¯•è·å–æ›´å¤šä¿¡æ¯
        github_repo = self._extract_github_repo(url)
        stars = 0

        if github_repo and self.deep_parse:
            owner, repo = github_repo
            repo_info = self._get_repo_info(owner, repo)

            if repo_info:
                # ä½¿ç”¨ä»“åº“æè¿°è¡¥å……ä¿¡æ¯
                if not description and repo_info.get("description"):
                    description = repo_info["description"]

                stars = repo_info.get("stargazers_count", 0)

                # æ£€æŸ¥æ˜¯å¦å·²å½’æ¡£
                if repo_info.get("archived", False):
                    return None

        return self.create_candidate_resource(
            url=url,
            title=title,
            description=description or title,
            author="",  # Awesome list é€šå¸¸ä¸æä¾›ä½œè€…ä¿¡æ¯
            author_url="",
            source_score=stars,  # ä½¿ç”¨ star æ•°ä½œä¸ºåˆ†æ•°
            extra_metadata={
                "source_list": source_list,
                "github_stars": stars,
            },
        )

    def _crawl_awesome_list(self, list_config: dict) -> List[dict]:
        """
        çˆ¬å–å•ä¸ª Awesome List / Crawl single Awesome List

        Args:
            list_config: åˆ—è¡¨é…ç½® / List configuration

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        list_name = list_config.get("name", "Unknown")
        list_url = list_config.get("url", "")
        keywords = list_config.get("keywords", [])

        print(f"      çˆ¬å– {list_name}...")

        # æå– GitHub repo ä¿¡æ¯
        github_repo = self._extract_github_repo(list_url)
        if not github_repo:
            print("         âš ï¸ æ— æ•ˆçš„ GitHub URL")
            return resources

        owner, repo = github_repo

        # è·å– README å†…å®¹
        readme_content = self._get_readme_content(owner, repo)
        if not readme_content:
            print("         âš ï¸ æ— æ³•è·å– README")
            return resources

        # è§£æé“¾æ¥
        links = self._parse_markdown_links(readme_content)
        print(f"         æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")

        # è¿‡æ»¤å’Œå¤„ç†é“¾æ¥
        processed_count = 0
        for title, url, description in links:
            if processed_count >= self.max_links_per_list:
                break

            if not self._filter_link(title, url, description, keywords):
                continue

            resource = self._create_resource_from_link(title, url, description, list_name)
            if resource:
                resources.append(resource)
                processed_count += 1

        print(f"         å‘ç° {len(resources)} ä¸ªç›¸å…³èµ„æº")

        return resources

    def crawl(self) -> List[dict]:
        """
        æ‰§è¡Œçˆ¬å– / Execute crawl

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        seen_urls = set()

        print(f"   ğŸ“‹ çˆ¬å– {len(self.lists)} ä¸ª Awesome Lists...")

        for list_config in self.lists:
            list_resources = self._crawl_awesome_list(list_config)

            for res in list_resources:
                url = res.get("PrimaryLink", "")
                if url not in seen_urls:
                    seen_urls.add(url)
                    resources.append(res)

        return resources
