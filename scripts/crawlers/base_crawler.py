#!/usr/bin/env python3
"""
çˆ¬è™«åŸºç±» / Base Crawler Class

æ‰€æœ‰çˆ¬è™«çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰é€šç”¨æ¥å£å’Œå·¥å…·æ–¹æ³•ã€‚
Abstract base class for all crawlers, defines common interface and utility methods.
"""

import csv
import hashlib
import json
import re
import time
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import requests
import yaml


class BaseCrawler(ABC):
    """çˆ¬è™«åŸºç±» / Base crawler class"""

    # é¡¹ç›®æ ¹ç›®å½•
    PROJECT_ROOT = Path(__file__).parent.parent.parent

    def __init__(self, config: dict, rate_limit_config: Optional[dict] = None):
        """
        åˆå§‹åŒ–çˆ¬è™« / Initialize crawler

        Args:
            config: çˆ¬è™«é…ç½® / Crawler configuration
            rate_limit_config: é€Ÿç‡é™åˆ¶é…ç½® / Rate limit configuration
        """
        self.config = config
        self.rate_limit_config = rate_limit_config or {}
        self.session = requests.Session()
        self.session.headers.update(
            {"User-Agent": "AwesomeClaudeCode-Bot/1.0 (+https://github.com/yiancode/AwesomeClaudeCode)"}
        )

        # åŠ è½½åˆ†ç±»é…ç½®
        self._categories_prefix = self._load_categories()

        # åŠ è½½å·²å­˜åœ¨çš„ URL
        self._existing_urls = self._load_existing_urls()

        # é€Ÿç‡é™åˆ¶çŠ¶æ€
        self._last_request_time = 0

    @property
    @abstractmethod
    def name(self) -> str:
        """çˆ¬è™«åç§° / Crawler name"""
        pass

    @property
    @abstractmethod
    def source_type(self) -> str:
        """æ•°æ®æºç±»å‹ / Data source type"""
        pass

    @abstractmethod
    def crawl(self) -> List[dict]:
        """
        æ‰§è¡Œçˆ¬å– / Execute crawl

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        pass

    def _load_categories(self) -> dict:
        """åŠ è½½åˆ†ç±»å®šä¹‰ / Load category definitions"""
        categories_file = self.PROJECT_ROOT / "templates" / "categories.yaml"
        with open(categories_file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        return {cat["id"]: cat["prefix"] for cat in data["categories"]}

    def _load_existing_urls(self) -> Set[str]:
        """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„èµ„æº URL / Load all existing resource URLs"""
        urls = set()

        # ä» CSV åŠ è½½
        csv_file = self.PROJECT_ROOT / "THE_RESOURCES_TABLE.csv"
        if csv_file.exists():
            with open(csv_file, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    url = self._normalize_url(row.get("PrimaryLink", ""))
                    if url:
                        urls.add(url)

        # ä» pending åŠ è½½
        pending_file = self.PROJECT_ROOT / "candidates" / "pending_resources.json"
        if pending_file.exists():
            with open(pending_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for res in data.get("resources", []):
                    url = self._normalize_url(res.get("PrimaryLink", ""))
                    if url:
                        urls.add(url)

        # ä» rejected åŠ è½½
        rejected_file = self.PROJECT_ROOT / "candidates" / "rejected_resources.json"
        if rejected_file.exists():
            with open(rejected_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for res in data.get("resources", []):
                    url = self._normalize_url(res.get("PrimaryLink", ""))
                    if url:
                        urls.add(url)

        return urls

    def _normalize_url(self, url: str) -> str:
        """è§„èŒƒåŒ– URL / Normalize URL"""
        if not url:
            return ""
        return url.strip().rstrip("/").lower()

    def _is_duplicate(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦å·²å­˜åœ¨ / Check if URL already exists"""
        normalized = self._normalize_url(url)
        return normalized in self._existing_urls

    def _rate_limit(self):
        """æ‰§è¡Œé€Ÿç‡é™åˆ¶ / Apply rate limiting"""
        min_interval = self.rate_limit_config.get("min_request_interval", 1.0)
        elapsed = time.time() - self._last_request_time

        if elapsed < min_interval:
            time.sleep(min_interval - elapsed)

        self._last_request_time = time.time()

    def _make_request(self, url: str, method: str = "GET", **kwargs) -> Optional[requests.Response]:
        """
        å‘èµ· HTTP è¯·æ±‚ï¼ˆå¸¦é€Ÿç‡é™åˆ¶ï¼‰
        Make HTTP request (with rate limiting)
        """
        self._rate_limit()

        timeout = kwargs.pop("timeout", 30)

        try:
            response = self.session.request(method, url, timeout=timeout, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   âš ï¸ è¯·æ±‚å¤±è´¥ [{url}]: {e}")
            return None

    def _extract_github_url(self, text: str) -> Optional[str]:
        """
        ä»æ–‡æœ¬ä¸­æå– GitHub URL
        Extract GitHub URL from text
        """
        # åŒ¹é… GitHub ä»“åº“ URL
        pattern = r"https?://github\.com/[\w\-]+/[\w\-\.]+"
        match = re.search(pattern, text)
        if match:
            url = match.group(0)
            # æ¸…ç† URLï¼ˆç§»é™¤ .git ç­‰åç¼€ï¼‰
            url = re.sub(r"\.git$", "", url)
            url = re.sub(r"[/\?#].*$", "", url)
            return url
        return None

    def _extract_urls(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰ URL
        Extract all URLs from text
        """
        pattern = r"https?://[^\s<>\"\'\)\]]+[^\s<>\"\'\)\]\.,;:!?]"
        urls = re.findall(pattern, text)
        return list(set(urls))

    def _is_relevant_url(self, url: str) -> bool:
        """
        æ£€æŸ¥ URL æ˜¯å¦ç›¸å…³ï¼ˆè¿‡æ»¤æ‰ä¸ç›¸å…³çš„é“¾æ¥ï¼‰
        Check if URL is relevant (filter out irrelevant links)
        """
        # æ’é™¤å¸¸è§çš„éèµ„æºé“¾æ¥
        excluded_domains = [
            "twitter.com",
            "x.com",
            "facebook.com",
            "linkedin.com",
            "youtube.com",
            "youtu.be",
            "reddit.com",
            "imgur.com",
            "medium.com",
            "dev.to",
            "news.ycombinator.com",
            "google.com",
            "bing.com",
            "amazon.com",
            "ebay.com",
        ]

        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        for excluded in excluded_domains:
            if excluded in domain:
                return False

        # ä¼˜å…ˆ GitHub é“¾æ¥
        if "github.com" in domain:
            return True

        # å…¶ä»–æŠ€æœ¯ç›¸å…³åŸŸå
        relevant_domains = [
            "gitlab.com",
            "bitbucket.org",
            "npmjs.com",
            "pypi.org",
            "crates.io",
            "pkg.go.dev",
            "anthropic.com",
            "claude.ai",
        ]

        for relevant in relevant_domains:
            if relevant in domain:
                return True

        return False

    def _generate_resource_id(self, category_id: str, url: str) -> str:
        """ç”Ÿæˆèµ„æº ID / Generate resource ID"""
        prefix = self._categories_prefix.get(category_id, "res")
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        return f"{prefix}-{url_hash}"

    def _infer_category(self, title: str, description: str, url: str) -> str:
        """
        æ¨æ–­èµ„æºåˆ†ç±» / Infer resource category

        Args:
            title: æ ‡é¢˜ / Title
            description: æè¿° / Description
            url: URL

        Returns:
            åˆ†ç±» ID / Category ID
        """
        combined = f"{title} {description}".lower()

        # åŸºäºå…³é”®è¯æ¨æ–­åˆ†ç±»
        if "mcp" in combined or "model context protocol" in combined:
            return "mcp-servers"
        if "hook" in combined:
            return "hooks"
        if "slash" in combined or "command" in combined:
            return "slash-commands"
        if "workflow" in combined or "guide" in combined:
            return "workflows"
        if "tool" in combined or "extension" in combined or "plugin" in combined:
            return "tooling"
        if "skill" in combined:
            return "skills"
        if "status" in combined or "statusline" in combined:
            return "statusline"
        if "claude.md" in combined:
            return "claude-md-files"
        if "client" in combined or "terminal" in combined or "cli" in combined:
            return "alternative-clients"

        return "ecosystem"

    def _calculate_relevance_score(
        self,
        title: str,
        description: str,
        url: str,
        score: int = 0,  # æ¥æºå¹³å°çš„åˆ†æ•°ï¼ˆå¦‚ Reddit upvotesï¼‰
    ) -> int:
        """
        è®¡ç®—ç›¸å…³æ€§è¯„åˆ† / Calculate relevance score

        Returns:
            ç›¸å…³æ€§è¯„åˆ† (0-100) / Relevance score (0-100)
        """
        relevance = 0
        combined = f"{title} {description}".lower()

        # é«˜ç›¸å…³æ€§å…³é”®è¯
        high_keywords = ["claude code", "claude-code", "anthropic", "mcp server", "model context protocol"]
        for keyword in high_keywords:
            if keyword in combined:
                relevance += 25

        # ä¸­ç­‰ç›¸å…³æ€§å…³é”®è¯
        medium_keywords = ["claude", "mcp", "llm tool", "ai assistant", "ai coding"]
        for keyword in medium_keywords:
            if keyword in combined:
                relevance += 15

        # GitHub é“¾æ¥åŠ åˆ†
        if "github.com" in url:
            relevance += 10

        # åŸºäºæ¥æºå¹³å°åˆ†æ•°åŠ åˆ†
        if score >= 100:
            relevance += 20
        elif score >= 50:
            relevance += 15
        elif score >= 20:
            relevance += 10
        elif score >= 10:
            relevance += 5

        return min(100, relevance)

    def create_candidate_resource(
        self,
        url: str,
        title: str,
        description: str,
        author: str = "",
        author_url: str = "",
        source_score: int = 0,
        extra_metadata: Optional[dict] = None,
    ) -> dict:
        """
        åˆ›å»ºå€™é€‰èµ„æº / Create candidate resource

        Args:
            url: èµ„æº URL / Resource URL
            title: æ ‡é¢˜ / Title
            description: æè¿° / Description
            author: ä½œè€… / Author
            author_url: ä½œè€…ä¸»é¡µ / Author URL
            source_score: æ¥æºå¹³å°åˆ†æ•° / Source platform score
            extra_metadata: é¢å¤–å…ƒæ•°æ® / Extra metadata

        Returns:
            å€™é€‰èµ„æºå­—å…¸ / Candidate resource dict
        """
        category_id = self._infer_category(title, description, url)
        resource_id = self._generate_resource_id(category_id, url)
        relevance_score = self._calculate_relevance_score(title, description, url, source_score)

        today = datetime.now().strftime("%Y/%m/%d")

        # æˆªæ–­è¿‡é•¿çš„æè¿°
        if len(description) > 200:
            description = description[:197] + "..."

        resource = {
            "ID": resource_id,
            "DisplayName": title,
            "DisplayName_ZH": title,  # éœ€è¦äººå·¥ç¿»è¯‘
            "Category": category_id,
            "SubCategory": "general",
            "PrimaryLink": url,
            "SecondaryLink": "",
            "Author": author,
            "AuthorProfile": author_url,
            "IsActive": "TRUE",
            "DateAdded": today,
            "LastModified": today,
            "LastChecked": today,
            "License": "",
            "Description": description,
            "Description_ZH": "",  # éœ€è¦äººå·¥ç¿»è¯‘
            "Tags_ZH": "",
            "IsPinned": "FALSE",
            "Section": "community",
            # å…ƒæ•°æ®
            "_source": self.source_type,
            "_source_crawler": self.name,
            "_discovered_at": datetime.now().isoformat(),
            "_status": "pending",
            "_relevance_score": relevance_score,
            "_source_score": source_score,
        }

        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if extra_metadata:
            for key, value in extra_metadata.items():
                resource[f"_{key}"] = value

        return resource

    def save_to_pending(self, resources: List[dict]) -> int:
        """
        ä¿å­˜èµ„æºåˆ°å¾…å®¡æ ¸é˜Ÿåˆ— / Save resources to pending queue

        Args:
            resources: èµ„æºåˆ—è¡¨ / List of resources

        Returns:
            æ·»åŠ çš„èµ„æºæ•°é‡ / Number of resources added
        """
        pending_file = self.PROJECT_ROOT / "candidates" / "pending_resources.json"

        if pending_file.exists():
            with open(pending_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {"_comment": "å€™é€‰èµ„æºé˜Ÿåˆ— - å¾…å®¡æ ¸çš„èµ„æº", "_schema_version": "1.0", "resources": []}

        added_count = 0
        for resource in resources:
            # å†æ¬¡æ£€æŸ¥é‡å¤
            url = self._normalize_url(resource.get("PrimaryLink", ""))
            if url and url not in self._existing_urls:
                data["resources"].append(resource)
                self._existing_urls.add(url)
                added_count += 1

        with open(pending_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        return added_count

    def run(self, dry_run: bool = False, limit: int = 10) -> Tuple[int, int]:
        """
        è¿è¡Œçˆ¬è™« / Run crawler

        Args:
            dry_run: æ˜¯å¦ä¸ºæ¼”ç¤ºæ¨¡å¼ / Whether in dry run mode
            limit: æœ€å¤§èµ„æºæ•°é‡ / Maximum number of resources

        Returns:
            (å‘ç°æ•°é‡, æ·»åŠ æ•°é‡) / (discovered count, added count)
        """
        print(f"\nğŸ•·ï¸  è¿è¡Œ {self.name} çˆ¬è™«...")

        try:
            resources = self.crawl()
        except Exception as e:
            print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
            return 0, 0

        # è¿‡æ»¤é‡å¤
        unique_resources = []
        for res in resources:
            url = res.get("PrimaryLink", "")
            if not self._is_duplicate(url):
                unique_resources.append(res)

        # æŒ‰ç›¸å…³æ€§æ’åº
        unique_resources.sort(key=lambda x: x.get("_relevance_score", 0), reverse=True)

        # é™åˆ¶æ•°é‡
        unique_resources = unique_resources[:limit]

        discovered_count = len(unique_resources)
        print(f"   ğŸ“Š å‘ç° {discovered_count} ä¸ªæ–°èµ„æº")

        if not unique_resources:
            return 0, 0

        # æ˜¾ç¤ºå‘ç°çš„èµ„æº
        for res in unique_resources:
            print(f"   ğŸ“Œ {res['DisplayName']}")
            print(f"      URL: {res['PrimaryLink']}")
            print(f"      åˆ†ç±»: {res['Category']}")
            print(f"      ç›¸å…³æ€§: {res.get('_relevance_score', 0)}/100")

        if dry_run:
            print("   [Dry Run] è·³è¿‡ä¿å­˜")
            return discovered_count, 0

        # ä¿å­˜åˆ°å¾…å®¡æ ¸é˜Ÿåˆ—
        added_count = self.save_to_pending(unique_resources)
        print(f"   âœ… å·²æ·»åŠ  {added_count} ä¸ªèµ„æºåˆ°å€™é€‰é˜Ÿåˆ—")

        return discovered_count, added_count
