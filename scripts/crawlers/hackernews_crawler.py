#!/usr/bin/env python3
"""
Hacker News çˆ¬è™« / Hacker News Crawler

ä» Hacker News æœç´¢å’Œå‘ç°ä¸ Claude Code ç›¸å…³çš„èµ„æºã€‚
Discovers Claude Code related resources from Hacker News search.

ä½¿ç”¨ Algolia HN Search APIã€‚
Uses Algolia HN Search API.
"""

from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
from urllib.parse import quote_plus

from .base_crawler import BaseCrawler


class HackerNewsCrawler(BaseCrawler):
    """Hacker News çˆ¬è™« / Hacker News crawler"""

    # Algolia HN Search API
    SEARCH_API = "https://hn.algolia.com/api/v1/search"
    SEARCH_BY_DATE_API = "https://hn.algolia.com/api/v1/search_by_date"

    @property
    def name(self) -> str:
        return "Hacker News"

    @property
    def source_type(self) -> str:
        return "hackernews"

    def __init__(self, config: dict, rate_limit_config: Optional[dict] = None):
        super().__init__(config, rate_limit_config)

        # HN ç‰¹å®šé…ç½®
        self.hn_config = config.get('hackernews', {})
        self.keywords = self.hn_config.get('keywords', ['claude code'])
        self.search_type = self.hn_config.get('search_type', 'story')  # story, comment, all
        self.min_score = self.hn_config.get('min_score', 5)
        self.results_per_keyword = self.hn_config.get('results_per_keyword', 20)
        self.max_age_days = self.hn_config.get('max_age_days', 30)
        self.sort_by = self.hn_config.get('sort_by', 'popularity')  # popularity, date

    def _search(self, query: str) -> List[dict]:
        """
        æ‰§è¡Œæœç´¢ / Execute search

        Args:
            query: æœç´¢æŸ¥è¯¢ / Search query

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ / List of search results
        """
        results = []

        # é€‰æ‹© API ç«¯ç‚¹
        if self.sort_by == 'date':
            api_url = self.SEARCH_BY_DATE_API
        else:
            api_url = self.SEARCH_API

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'query': query,
            'hitsPerPage': self.results_per_keyword,
        }

        # è®¾ç½®æœç´¢ç±»å‹
        if self.search_type == 'story':
            params['tags'] = 'story'
        elif self.search_type == 'comment':
            params['tags'] = 'comment'
        # 'all' ä¸éœ€è¦ tags å‚æ•°

        # è®¾ç½®æ—¶é—´èŒƒå›´
        if self.max_age_days:
            timestamp = int((datetime.now(timezone.utc) - timedelta(days=self.max_age_days)).timestamp())
            params['numericFilters'] = f'created_at_i>{timestamp}'

        response = self._make_request(api_url, params=params)

        if not response:
            return results

        try:
            data = response.json()
            hits = data.get('hits', [])

            for hit in hits:
                results.append(hit)

        except Exception as e:
            print(f"      âš ï¸ è§£æå“åº”å¤±è´¥: {e}")

        return results

    def _filter_hit(self, hit: dict) -> bool:
        """
        è¿‡æ»¤æœç´¢ç»“æœ / Filter search result

        Args:
            hit: æœç´¢ç»“æœ / Search result

        Returns:
            æ˜¯å¦é€šè¿‡è¿‡æ»¤ / Whether passed filter
        """
        # æ£€æŸ¥åˆ†æ•°
        points = hit.get('points', 0) or 0
        if points < self.min_score:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰ URL
        url = hit.get('url', '')
        story_url = hit.get('story_url', '')
        target_url = url or story_url

        if not target_url:
            return False

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        github_url = self._extract_github_url(target_url)
        check_url = github_url or target_url

        if self._is_duplicate(check_url):
            return False

        return True

    def _create_resource_from_hit(self, hit: dict) -> Optional[dict]:
        """
        ä»æœç´¢ç»“æœåˆ›å»ºèµ„æº / Create resource from search result

        Args:
            hit: æœç´¢ç»“æœ / Search result

        Returns:
            å€™é€‰èµ„æºæˆ– None / Candidate resource or None
        """
        title = hit.get('title', '') or hit.get('story_title', '')
        url = hit.get('url', '') or hit.get('story_url', '')
        author = hit.get('author', '')
        points = hit.get('points', 0) or 0
        num_comments = hit.get('num_comments', 0) or 0
        object_id = hit.get('objectID', '')
        created_at = hit.get('created_at', '')

        # ä¼˜å…ˆæå– GitHub é“¾æ¥
        github_url = self._extract_github_url(url)
        target_url = github_url or url

        if not target_url or not self._is_relevant_url(target_url):
            return None

        # æ„å»ºæè¿°
        description = title
        if hit.get('story_text'):
            # å¦‚æœæ˜¯è¯„è®ºï¼Œä½¿ç”¨è¯„è®ºæ–‡æœ¬
            description = hit['story_text'][:500]

        return self.create_candidate_resource(
            url=target_url,
            title=title,
            description=description,
            author=author,
            author_url=f"https://news.ycombinator.com/user?id={author}" if author else '',
            source_score=points,
            extra_metadata={
                'hn_id': object_id,
                'hn_url': f"https://news.ycombinator.com/item?id={object_id}",
                'hn_points': points,
                'hn_comments': num_comments,
                'original_url': url,
                'created_at': created_at,
            }
        )

    def crawl(self) -> List[dict]:
        """
        æ‰§è¡Œçˆ¬å– / Execute crawl

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        seen_urls = set()

        print(f"   ğŸ“‹ æœç´¢ {len(self.keywords)} ä¸ªå…³é”®è¯...")

        for keyword in self.keywords:
            print(f"      æœç´¢ \"{keyword}\"...")

            hits = self._search(keyword)
            print(f"         æ‰¾åˆ° {len(hits)} ä¸ªç»“æœ")

            for hit in hits:
                if not self._filter_hit(hit):
                    continue

                resource = self._create_resource_from_hit(hit)
                if resource:
                    url = resource.get('PrimaryLink', '')
                    if url not in seen_urls:
                        seen_urls.add(url)
                        resources.append(resource)

        return resources
