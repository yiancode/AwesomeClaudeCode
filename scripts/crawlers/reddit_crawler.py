#!/usr/bin/env python3
"""
Reddit çˆ¬è™« / Reddit Crawler

ä» Reddit æœç´¢å’Œå‘ç°ä¸ Claude Code ç›¸å…³çš„èµ„æºã€‚
Discovers Claude Code related resources from Reddit search.

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. ä½¿ç”¨ Reddit APIï¼ˆéœ€è¦ OAuthï¼‰
2. ä½¿ç”¨å…¬å¼€ JSON ç«¯ç‚¹ï¼ˆæ— éœ€è®¤è¯ï¼Œä½†æœ‰é€Ÿç‡é™åˆ¶ï¼‰
"""

import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import quote_plus

from .base_crawler import BaseCrawler


class RedditCrawler(BaseCrawler):
    """Reddit çˆ¬è™« / Reddit crawler"""

    @property
    def name(self) -> str:
        return "Reddit"

    @property
    def source_type(self) -> str:
        return "reddit"

    def __init__(self, config: dict, rate_limit_config: Optional[dict] = None):
        super().__init__(config, rate_limit_config)

        # Reddit ç‰¹å®šé…ç½®
        self.reddit_config = config.get('reddit', {})
        self.subreddits = self.reddit_config.get('subreddits', ['ClaudeAI'])
        self.keywords = self.reddit_config.get('keywords', ['claude code'])
        self.min_score = self.reddit_config.get('min_score', 10)
        self.max_age_days = self.reddit_config.get('max_age_days', 30)
        self.posts_per_subreddit = self.reddit_config.get('posts_per_subreddit', 25)
        self.sort = self.reddit_config.get('sort', 'relevance')
        self.time_filter = self.reddit_config.get('time_filter', 'month')

        # æ£€æŸ¥æ˜¯å¦æœ‰ Reddit API å‡­è¯
        self.client_id = os.environ.get('REDDIT_CLIENT_ID')
        self.client_secret = os.environ.get('REDDIT_CLIENT_SECRET')
        self.use_api = bool(self.client_id and self.client_secret)

        if self.use_api:
            self._setup_oauth()

    def _setup_oauth(self):
        """è®¾ç½® Reddit OAuth / Setup Reddit OAuth"""
        try:
            auth = (self.client_id, self.client_secret)
            data = {
                'grant_type': 'client_credentials',
                'device_id': 'DO_NOT_TRACK_THIS_DEVICE'
            }
            headers = {'User-Agent': self.session.headers['User-Agent']}

            response = self.session.post(
                'https://www.reddit.com/api/v1/access_token',
                auth=auth,
                data=data,
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                token_data = response.json()
                access_token = token_data.get('access_token')
                self.session.headers['Authorization'] = f'Bearer {access_token}'
                print("   âœ… Reddit OAuth è®¤è¯æˆåŠŸ")
            else:
                print(f"   âš ï¸ Reddit OAuth å¤±è´¥ï¼Œä½¿ç”¨å…¬å¼€ç«¯ç‚¹")
                self.use_api = False

        except Exception as e:
            print(f"   âš ï¸ Reddit OAuth é”™è¯¯: {e}")
            self.use_api = False

    def _search_subreddit(self, subreddit: str, query: str) -> List[dict]:
        """
        æœç´¢ç‰¹å®š subreddit / Search specific subreddit

        Args:
            subreddit: Subreddit åç§° / Subreddit name
            query: æœç´¢æŸ¥è¯¢ / Search query

        Returns:
            å¸–å­åˆ—è¡¨ / List of posts
        """
        posts = []

        if self.use_api:
            url = f"https://oauth.reddit.com/r/{subreddit}/search"
        else:
            url = f"https://www.reddit.com/r/{subreddit}/search.json"

        params = {
            'q': query,
            'sort': self.sort,
            't': self.time_filter,
            'limit': self.posts_per_subreddit,
            'restrict_sr': 'true',
        }

        response = self._make_request(url, params=params)

        if not response:
            return posts

        try:
            data = response.json()
            children = data.get('data', {}).get('children', [])

            for child in children:
                post_data = child.get('data', {})
                posts.append(post_data)

        except Exception as e:
            print(f"      âš ï¸ è§£æå“åº”å¤±è´¥: {e}")

        return posts

    def _get_hot_posts(self, subreddit: str) -> List[dict]:
        """
        è·å–çƒ­é—¨å¸–å­ / Get hot posts

        Args:
            subreddit: Subreddit åç§° / Subreddit name

        Returns:
            å¸–å­åˆ—è¡¨ / List of posts
        """
        posts = []

        if self.use_api:
            url = f"https://oauth.reddit.com/r/{subreddit}/hot"
        else:
            url = f"https://www.reddit.com/r/{subreddit}/hot.json"

        params = {
            'limit': self.posts_per_subreddit,
        }

        response = self._make_request(url, params=params)

        if not response:
            return posts

        try:
            data = response.json()
            children = data.get('data', {}).get('children', [])

            for child in children:
                post_data = child.get('data', {})
                posts.append(post_data)

        except Exception as e:
            print(f"      âš ï¸ è§£æå“åº”å¤±è´¥: {e}")

        return posts

    def _filter_post(self, post: dict) -> bool:
        """
        è¿‡æ»¤å¸–å­ / Filter post

        Args:
            post: å¸–å­æ•°æ® / Post data

        Returns:
            æ˜¯å¦é€šè¿‡è¿‡æ»¤ / Whether passed filter
        """
        # æ£€æŸ¥åˆ†æ•°
        score = post.get('score', 0)
        if score < self.min_score:
            return False

        # æ£€æŸ¥å¹´é¾„
        created_utc = post.get('created_utc', 0)
        if created_utc:
            post_date = datetime.fromtimestamp(created_utc)
            max_age = datetime.now() - timedelta(days=self.max_age_days)
            if post_date < max_age:
                return False

        # æ’é™¤è‡ªæˆ‘æ¨å¹¿/å¹¿å‘Šç±»
        if post.get('is_self', False) and not post.get('selftext'):
            return False

        return True

    def _extract_resource_from_post(self, post: dict) -> Optional[dict]:
        """
        ä»å¸–å­ä¸­æå–èµ„æº / Extract resource from post

        Args:
            post: å¸–å­æ•°æ® / Post data

        Returns:
            å€™é€‰èµ„æºæˆ– None / Candidate resource or None
        """
        title = post.get('title', '')
        selftext = post.get('selftext', '')
        url = post.get('url', '')
        score = post.get('score', 0)
        author = post.get('author', '')
        permalink = post.get('permalink', '')

        # ä¼˜å…ˆæå– GitHub é“¾æ¥
        github_url = self._extract_github_url(f"{title} {selftext} {url}")

        if github_url and not self._is_duplicate(github_url):
            # æ¸…ç†æè¿°
            description = selftext[:500] if selftext else title

            return self.create_candidate_resource(
                url=github_url,
                title=title,
                description=description,
                author=author,
                author_url=f"https://reddit.com/u/{author}" if author else '',
                source_score=score,
                extra_metadata={
                    'reddit_permalink': f"https://reddit.com{permalink}",
                    'reddit_score': score,
                    'subreddit': post.get('subreddit', ''),
                }
            )

        # å¦‚æœå¸–å­ URL æœ¬èº«æ˜¯ç›¸å…³é“¾æ¥
        if url and self._is_relevant_url(url) and not self._is_duplicate(url):
            description = selftext[:500] if selftext else title

            return self.create_candidate_resource(
                url=url,
                title=title,
                description=description,
                author=author,
                author_url=f"https://reddit.com/u/{author}" if author else '',
                source_score=score,
                extra_metadata={
                    'reddit_permalink': f"https://reddit.com{permalink}",
                    'reddit_score': score,
                    'subreddit': post.get('subreddit', ''),
                }
            )

        return None

    def crawl(self) -> List[dict]:
        """
        æ‰§è¡Œçˆ¬å– / Execute crawl

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        seen_urls = set()

        print(f"   ğŸ“‹ æœç´¢ {len(self.subreddits)} ä¸ª subreddits...")

        # æŒ‰å…³é”®è¯æœç´¢
        for subreddit in self.subreddits:
            print(f"      æœç´¢ r/{subreddit}...")

            for keyword in self.keywords:
                posts = self._search_subreddit(subreddit, keyword)

                for post in posts:
                    if not self._filter_post(post):
                        continue

                    resource = self._extract_resource_from_post(post)
                    if resource:
                        url = resource.get('PrimaryLink', '')
                        if url not in seen_urls:
                            seen_urls.add(url)
                            resources.append(resource)

            # ä¹Ÿè·å–çƒ­é—¨å¸–å­
            hot_posts = self._get_hot_posts(subreddit)
            for post in hot_posts:
                if not self._filter_post(post):
                    continue

                # æ£€æŸ¥æ ‡é¢˜/å†…å®¹æ˜¯å¦åŒ…å«å…³é”®è¯
                title = post.get('title', '').lower()
                selftext = post.get('selftext', '').lower()
                combined = f"{title} {selftext}"

                has_keyword = any(kw.lower() in combined for kw in self.keywords)
                if not has_keyword:
                    continue

                resource = self._extract_resource_from_post(post)
                if resource:
                    url = resource.get('PrimaryLink', '')
                    if url not in seen_urls:
                        seen_urls.add(url)
                        resources.append(resource)

        return resources
