#!/usr/bin/env python3
"""
RSS çˆ¬è™« / RSS Crawler

ä» RSS è®¢é˜…æºä¸­å‘ç°ä¸ Claude Code ç›¸å…³çš„èµ„æºã€‚
Discovers Claude Code related resources from RSS feeds.
"""

import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
from email.utils import parsedate_to_datetime

try:
    import feedparser

    HAS_FEEDPARSER = True
except ImportError:
    HAS_FEEDPARSER = False

from .base_crawler import BaseCrawler


class RSSCrawler(BaseCrawler):
    """RSS çˆ¬è™« / RSS crawler"""

    @property
    def name(self) -> str:
        return "RSS"

    @property
    def source_type(self) -> str:
        return "rss"

    def __init__(self, config: dict, rate_limit_config: Optional[dict] = None):
        super().__init__(config, rate_limit_config)

        # RSS ç‰¹å®šé…ç½®
        self.rss_config = config.get("rss", {})
        self.feeds = self.rss_config.get("feeds", [])
        self.entries_per_feed = self.rss_config.get("entries_per_feed", 30)
        self.max_age_days = self.rss_config.get("max_age_days", 14)

        if not HAS_FEEDPARSER:
            print("   âš ï¸ feedparser æœªå®‰è£…ï¼ŒRSS çˆ¬è™«åŠŸèƒ½å—é™")

    def _parse_feed(self, feed_url: str) -> List[dict]:
        """
        è§£æ RSS feed / Parse RSS feed

        Args:
            feed_url: Feed URL

        Returns:
            æ¡ç›®åˆ—è¡¨ / List of entries
        """
        if HAS_FEEDPARSER:
            return self._parse_with_feedparser(feed_url)
        else:
            return self._parse_with_requests(feed_url)

    def _parse_with_feedparser(self, feed_url: str) -> List[dict]:
        """ä½¿ç”¨ feedparser è§£æ / Parse with feedparser"""
        entries = []

        try:
            feed = feedparser.parse(feed_url)

            for entry in feed.entries[: self.entries_per_feed]:
                entries.append(
                    {
                        "title": entry.get("title", ""),
                        "link": entry.get("link", ""),
                        "description": entry.get("summary", entry.get("description", "")),
                        "published": entry.get("published", ""),
                        "author": entry.get("author", ""),
                    }
                )

        except Exception as e:
            print(f"      âš ï¸ è§£æ feed å¤±è´¥: {e}")

        return entries

    def _parse_with_requests(self, feed_url: str) -> List[dict]:
        """ä½¿ç”¨ requests æ‰‹åŠ¨è§£æ / Parse manually with requests"""
        entries = []

        response = self._make_request(feed_url)
        if not response:
            return entries

        content = response.text

        # ç®€å•çš„ XML è§£æï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
        # åŒ¹é… <item> æˆ– <entry> æ ‡ç­¾
        item_pattern = r"<(?:item|entry)>(.*?)</(?:item|entry)>"
        items = re.findall(item_pattern, content, re.DOTALL)

        for item in items[: self.entries_per_feed]:
            title = self._extract_xml_value(item, "title")
            link = self._extract_xml_value(item, "link")
            description = (
                self._extract_xml_value(item, "description")
                or self._extract_xml_value(item, "summary")
                or self._extract_xml_value(item, "content")
            )
            published = (
                self._extract_xml_value(item, "pubDate")
                or self._extract_xml_value(item, "published")
                or self._extract_xml_value(item, "updated")
            )
            author = self._extract_xml_value(item, "author") or self._extract_xml_value(item, "dc:creator")

            # å¤„ç† Atom æ ¼å¼çš„ link
            if not link:
                link_match = re.search(r'<link[^>]*href=["\']([^"\']+)["\']', item)
                if link_match:
                    link = link_match.group(1)

            if title and link:
                entries.append(
                    {
                        "title": self._clean_html(title),
                        "link": link,
                        "description": self._clean_html(description),
                        "published": published,
                        "author": self._clean_html(author),
                    }
                )

        return entries

    def _extract_xml_value(self, xml: str, tag: str) -> str:
        """ä» XML ä¸­æå–æ ‡ç­¾å€¼ / Extract tag value from XML"""
        # å¤„ç† CDATA
        pattern = rf"<{tag}[^>]*>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</{tag}>"
        match = re.search(pattern, xml, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return ""

    def _clean_html(self, text: str) -> str:
        """æ¸…ç† HTML æ ‡ç­¾ / Clean HTML tags"""
        if not text:
            return ""
        # ç§»é™¤ HTML æ ‡ç­¾
        clean = re.sub(r"<[^>]+>", "", text)
        # è§£ç  HTML å®ä½“
        clean = clean.replace("&amp;", "&")
        clean = clean.replace("&lt;", "<")
        clean = clean.replace("&gt;", ">")
        clean = clean.replace("&quot;", '"')
        clean = clean.replace("&#39;", "'")
        clean = clean.replace("&nbsp;", " ")
        # æ¸…ç†å¤šä½™ç©ºç™½
        clean = re.sub(r"\s+", " ", clean).strip()
        return clean

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸² / Parse date string"""
        if not date_str:
            return None

        try:
            # å°è¯• RFC 2822 æ ¼å¼
            return parsedate_to_datetime(date_str)
        except Exception:
            pass

        try:
            # å°è¯• ISO æ ¼å¼
            return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        except Exception:
            pass

        return None

    def _filter_entry(self, entry: dict, keywords: List[str]) -> bool:
        """
        è¿‡æ»¤æ¡ç›® / Filter entry

        Args:
            entry: æ¡ç›®æ•°æ® / Entry data
            keywords: å…³é”®è¯åˆ—è¡¨ / Keyword list

        Returns:
            æ˜¯å¦é€šè¿‡è¿‡æ»¤ / Whether passed filter
        """
        title = entry.get("title", "")
        description = entry.get("description", "")
        link = entry.get("link", "")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        github_url = self._extract_github_url(f"{title} {description} {link}")
        target_url = github_url or link

        if self._is_duplicate(target_url):
            return False

        # æ£€æŸ¥å¹´é¾„
        published = entry.get("published", "")
        if published:
            pub_date = self._parse_date(published)
            if pub_date:
                # ç¡®ä¿æ—¶åŒºæ„ŸçŸ¥
                now = datetime.now(timezone.utc)
                if pub_date.tzinfo is None:
                    pub_date = pub_date.replace(tzinfo=timezone.utc)

                max_age = now - timedelta(days=self.max_age_days)
                if pub_date < max_age:
                    return False

        # å¦‚æœæœ‰å…³é”®è¯é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
        if keywords:
            combined = f"{title} {description}".lower()
            has_keyword = any(kw.lower() in combined for kw in keywords)
            if not has_keyword:
                return False

        return True

    def _create_resource_from_entry(self, entry: dict, feed_name: str) -> Optional[dict]:
        """
        ä»æ¡ç›®åˆ›å»ºèµ„æº / Create resource from entry

        Args:
            entry: æ¡ç›®æ•°æ® / Entry data
            feed_name: Feed åç§° / Feed name

        Returns:
            å€™é€‰èµ„æºæˆ– None / Candidate resource or None
        """
        title = entry.get("title", "")
        description = entry.get("description", "")
        link = entry.get("link", "")
        author = entry.get("author", "")

        # ä¼˜å…ˆæå– GitHub é“¾æ¥
        github_url = self._extract_github_url(f"{title} {description} {link}")
        target_url = github_url or link

        if not target_url or not self._is_relevant_url(target_url):
            # å¦‚æœä¸æ˜¯ç›¸å…³é“¾æ¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ GitHub é“¾æ¥
            if not github_url:
                return None
            target_url = github_url

        return self.create_candidate_resource(
            url=target_url,
            title=title,
            description=description or title,
            author=author,
            author_url="",
            source_score=0,
            extra_metadata={
                "rss_feed": feed_name,
                "original_link": link,
                "published": entry.get("published", ""),
            },
        )

    def _crawl_feed(self, feed_config: dict) -> List[dict]:
        """
        çˆ¬å–å•ä¸ª feed / Crawl single feed

        Args:
            feed_config: Feed é…ç½® / Feed configuration

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        feed_name = feed_config.get("name", "Unknown")
        feed_url = feed_config.get("url", "")
        keywords = feed_config.get("keywords", [])

        print(f"      çˆ¬å– {feed_name}...")

        entries = self._parse_feed(feed_url)
        print(f"         è·å– {len(entries)} ä¸ªæ¡ç›®")

        for entry in entries:
            if not self._filter_entry(entry, keywords):
                continue

            resource = self._create_resource_from_entry(entry, feed_name)
            if resource:
                resources.append(resource)

        print(f"         å‘ç° {len(resources)} ä¸ªç›¸å…³èµ„æº")

        return resources

    def crawl(self) -> List[dict]:
        """
        æ‰§è¡Œçˆ¬å– / Execute crawl

        Returns:
            å‘ç°çš„èµ„æºåˆ—è¡¨ / List of discovered resources
        """
        resources = []
        seen_urls = set()

        print(f"   ğŸ“‹ çˆ¬å– {len(self.feeds)} ä¸ª RSS feeds...")

        for feed_config in self.feeds:
            feed_resources = self._crawl_feed(feed_config)

            for res in feed_resources:
                url = res.get("PrimaryLink", "")
                if url not in seen_urls:
                    seen_urls.add(url)
                    resources.append(res)

        return resources
