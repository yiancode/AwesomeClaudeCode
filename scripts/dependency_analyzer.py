#!/usr/bin/env python3
"""
ä¾èµ–åˆ†æè„šæœ¬ / Dependency Analyzer Script

åˆ†æç°æœ‰èµ„æºçš„ä¾èµ–å…³ç³»ï¼Œå‘ç°ç›¸å…³çš„æ–°èµ„æºï¼š
1. åˆ†æ package.json / requirements.txt / Cargo.toml ç­‰ä¾èµ–æ–‡ä»¶
2. å‘ç°å¸¸ç”¨çš„ç›¸å…³åº“
3. è¯†åˆ«ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ ¸å¿ƒä¾èµ–

ç”¨æ³• / Usage:
    python scripts/dependency_analyzer.py [--analyze] [--discover]
"""

import argparse
import csv
import hashlib
import json
import os
import re
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import requests
import yaml

# é¡¹ç›®æ ¹ç›®å½• / Project root
PROJECT_ROOT = Path(__file__).parent.parent


def load_categories() -> dict:
    """åŠ è½½åˆ†ç±»å®šä¹‰ / Load category definitions"""
    categories_file = PROJECT_ROOT / "templates" / "categories.yaml"
    with open(categories_file, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    return {cat['id']: cat['prefix'] for cat in data['categories']}


def load_existing_resources() -> List[dict]:
    """åŠ è½½ç°æœ‰èµ„æº / Load existing resources"""
    resources = []
    csv_file = PROJECT_ROOT / 'THE_RESOURCES_TABLE.csv'

    if csv_file.exists():
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                resources.append(row)

    return resources


def load_existing_urls() -> Set[str]:
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„ URL / Load all existing URLs"""
    urls = set()

    # ä» CSV åŠ è½½
    csv_file = PROJECT_ROOT / 'THE_RESOURCES_TABLE.csv'
    if csv_file.exists():
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get('PrimaryLink', '').strip().rstrip('/').lower()
                if url:
                    urls.add(url)

    # ä» pending åŠ è½½
    pending_file = PROJECT_ROOT / 'candidates' / 'pending_resources.json'
    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for res in data.get('resources', []):
                url = res.get('PrimaryLink', '').strip().rstrip('/').lower()
                if url:
                    urls.add(url)

    return urls


def extract_github_info(url: str) -> Optional[Tuple[str, str]]:
    """ä» URL æå– GitHub owner/repo / Extract GitHub owner/repo from URL"""
    if 'github.com' not in url:
        return None

    parts = url.rstrip('/').split('/')
    try:
        github_index = next(i for i, p in enumerate(parts) if 'github.com' in p)
        if len(parts) > github_index + 2:
            owner = parts[github_index + 1]
            repo = parts[github_index + 2].replace('.git', '')
            return (owner, repo)
    except (StopIteration, IndexError):
        pass

    return None


class DependencyAnalyzer:
    """ä¾èµ–åˆ†æå™¨ / Dependency Analyzer"""

    # ç›¸å…³çš„åŒ…åæ¨¡å¼
    RELEVANT_PATTERNS = [
        r'anthropic',
        r'claude',
        r'mcp[-_]?',
        r'model[-_]?context[-_]?protocol',
        r'llm[-_]?',
        r'ai[-_]?assistant',
    ]

    def __init__(self):
        self.github_token = os.environ.get('GITHUB_TOKEN')
        self.session = requests.Session()
        self.session.headers['User-Agent'] = 'AwesomeClaudeCode-Bot/1.0'

        if self.github_token:
            self.session.headers['Authorization'] = f'Bearer {self.github_token}'

        self.categories_prefix = load_categories()
        self.existing_urls = load_existing_urls()

        # ä¾èµ–ç»Ÿè®¡
        self.dependency_counts = Counter()
        self.dependency_sources = defaultdict(list)

    def _get_file_content(self, owner: str, repo: str, path: str) -> Optional[str]:
        """
        è·å–ä»“åº“æ–‡ä»¶å†…å®¹ / Get repository file content

        Args:
            owner: ä»“åº“æ‰€æœ‰è€… / Repository owner
            repo: ä»“åº“å / Repository name
            path: æ–‡ä»¶è·¯å¾„ / File path

        Returns:
            æ–‡ä»¶å†…å®¹æˆ– None / File content or None
        """
        url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{path}"
        try:
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                return response.text

            # å°è¯• master åˆ†æ”¯
            url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{path}"
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                return response.text

        except requests.exceptions.RequestException:
            pass

        return None

    def _parse_package_json(self, content: str) -> List[str]:
        """è§£æ package.json ä¾èµ– / Parse package.json dependencies"""
        dependencies = []

        try:
            data = json.loads(content)

            # åˆå¹¶æ‰€æœ‰ä¾èµ–
            for key in ['dependencies', 'devDependencies', 'peerDependencies']:
                deps = data.get(key, {})
                dependencies.extend(deps.keys())

        except json.JSONDecodeError:
            pass

        return dependencies

    def _parse_requirements_txt(self, content: str) -> List[str]:
        """è§£æ requirements.txt ä¾èµ– / Parse requirements.txt dependencies"""
        dependencies = []

        for line in content.split('\n'):
            line = line.strip()

            # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
            if not line or line.startswith('#'):
                continue

            # ç§»é™¤ç‰ˆæœ¬çº¦æŸ
            package = re.split(r'[<>=!~\[]', line)[0].strip()
            if package:
                dependencies.append(package)

        return dependencies

    def _parse_pyproject_toml(self, content: str) -> List[str]:
        """è§£æ pyproject.toml ä¾èµ– / Parse pyproject.toml dependencies"""
        dependencies = []

        # ç®€å•çš„ TOML è§£æ
        # æŸ¥æ‰¾ dependencies éƒ¨åˆ†
        dep_match = re.search(r'dependencies\s*=\s*\[(.*?)\]', content, re.DOTALL)
        if dep_match:
            deps_str = dep_match.group(1)
            # æå–åŒ…å
            packages = re.findall(r'"([^"]+)"', deps_str)
            for pkg in packages:
                # ç§»é™¤ç‰ˆæœ¬çº¦æŸ
                package = re.split(r'[<>=!~\[]', pkg)[0].strip()
                if package:
                    dependencies.append(package)

        return dependencies

    def _parse_cargo_toml(self, content: str) -> List[str]:
        """è§£æ Cargo.toml ä¾èµ– / Parse Cargo.toml dependencies"""
        dependencies = []

        # æŸ¥æ‰¾ [dependencies] éƒ¨åˆ†
        in_deps = False
        for line in content.split('\n'):
            line = line.strip()

            if line.startswith('[dependencies]'):
                in_deps = True
                continue
            elif line.startswith('[') and in_deps:
                in_deps = False

            if in_deps and '=' in line:
                package = line.split('=')[0].strip()
                if package:
                    dependencies.append(package)

        return dependencies

    def _is_relevant_package(self, package: str) -> bool:
        """æ£€æŸ¥åŒ…æ˜¯å¦ç›¸å…³ / Check if package is relevant"""
        package_lower = package.lower()

        for pattern in self.RELEVANT_PATTERNS:
            if re.search(pattern, package_lower):
                return True

        return False

    def analyze_repository(self, owner: str, repo: str) -> List[str]:
        """
        åˆ†æå•ä¸ªä»“åº“çš„ä¾èµ– / Analyze dependencies of single repository

        Args:
            owner: ä»“åº“æ‰€æœ‰è€… / Repository owner
            repo: ä»“åº“å / Repository name

        Returns:
            ç›¸å…³ä¾èµ–åˆ—è¡¨ / List of relevant dependencies
        """
        relevant_deps = []

        # å°è¯•å„ç§ä¾èµ–æ–‡ä»¶
        dep_files = [
            ('package.json', self._parse_package_json),
            ('requirements.txt', self._parse_requirements_txt),
            ('pyproject.toml', self._parse_pyproject_toml),
            ('Cargo.toml', self._parse_cargo_toml),
        ]

        for filename, parser in dep_files:
            content = self._get_file_content(owner, repo, filename)
            if content:
                deps = parser(content)
                for dep in deps:
                    if self._is_relevant_package(dep):
                        relevant_deps.append(dep)
                        self.dependency_counts[dep] += 1
                        self.dependency_sources[dep].append(f"{owner}/{repo}")

        return relevant_deps

    def analyze_all_resources(self) -> Dict[str, List[str]]:
        """
        åˆ†ææ‰€æœ‰èµ„æºçš„ä¾èµ– / Analyze dependencies of all resources

        Returns:
            èµ„æºåˆ°ä¾èµ–çš„æ˜ å°„ / Mapping of resources to dependencies
        """
        resources = load_existing_resources()
        results = {}

        print(f"   åˆ†æ {len(resources)} ä¸ªèµ„æº...")

        for i, res in enumerate(resources):
            url = res.get('PrimaryLink', '')
            github_info = extract_github_info(url)

            if not github_info:
                continue

            owner, repo = github_info
            print(f"   [{i+1}/{len(resources)}] {owner}/{repo}...", end=' ')

            deps = self.analyze_repository(owner, repo)

            if deps:
                results[f"{owner}/{repo}"] = deps
                print(f"æ‰¾åˆ° {len(deps)} ä¸ªç›¸å…³ä¾èµ–")
            else:
                print("æ— ç›¸å…³ä¾èµ–")

        return results

    def get_popular_dependencies(self, min_count: int = 2) -> List[Tuple[str, int]]:
        """
        è·å–å¸¸ç”¨ä¾èµ– / Get popular dependencies

        Args:
            min_count: æœ€å°å‡ºç°æ¬¡æ•° / Minimum occurrence count

        Returns:
            ä¾èµ–åŠå…¶å‡ºç°æ¬¡æ•°åˆ—è¡¨ / List of dependencies and their counts
        """
        return [(dep, count) for dep, count in self.dependency_counts.most_common()
                if count >= min_count]

    def discover_related_packages(self) -> List[dict]:
        """
        å‘ç°ç›¸å…³çš„åŒ…/åº“ / Discover related packages/libraries

        Returns:
            å‘ç°çš„å€™é€‰èµ„æºåˆ—è¡¨ / List of discovered candidate resources
        """
        candidates = []

        # è·å–å¸¸ç”¨ä¾èµ–
        popular = self.get_popular_dependencies(min_count=2)

        print(f"\nğŸ“¦ å‘ç° {len(popular)} ä¸ªå¸¸ç”¨ç›¸å…³ä¾èµ–:")

        for dep, count in popular:
            print(f"   - {dep} (å‡ºç° {count} æ¬¡)")

            # å°è¯•æ‰¾åˆ°åŒ…çš„ GitHub ä»“åº“
            package_url = self._find_package_repo(dep)

            if package_url and package_url.lower() not in self.existing_urls:
                candidate = self._create_candidate(dep, package_url, count)
                if candidate:
                    candidates.append(candidate)

        return candidates

    def _find_package_repo(self, package: str) -> Optional[str]:
        """
        å°è¯•æ‰¾åˆ°åŒ…çš„ GitHub ä»“åº“ / Try to find package's GitHub repository

        Args:
            package: åŒ…å / Package name

        Returns:
            GitHub URL æˆ– None / GitHub URL or None
        """
        # å°è¯• npm
        npm_url = f"https://registry.npmjs.org/{package}"
        try:
            response = self.session.get(npm_url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                repo = data.get('repository', {})
                if isinstance(repo, dict):
                    url = repo.get('url', '')
                elif isinstance(repo, str):
                    url = repo
                else:
                    url = ''

                if 'github.com' in url:
                    # æ¸…ç† URL
                    url = re.sub(r'^git\+', '', url)
                    url = re.sub(r'\.git$', '', url)
                    url = re.sub(r'^git://', 'https://', url)
                    return url
        except Exception:
            pass

        # å°è¯• PyPI
        pypi_url = f"https://pypi.org/pypi/{package}/json"
        try:
            response = self.session.get(pypi_url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                info = data.get('info', {})
                project_urls = info.get('project_urls', {}) or {}

                # æ£€æŸ¥å„ç§å¯èƒ½çš„ URL
                for key in ['Repository', 'Source', 'Homepage', 'GitHub']:
                    url = project_urls.get(key, '')
                    if 'github.com' in url:
                        return url

                # æ£€æŸ¥ home_page
                home_page = info.get('home_page', '')
                if 'github.com' in home_page:
                    return home_page
        except Exception:
            pass

        return None

    def _create_candidate(self, package: str, url: str, usage_count: int) -> Optional[dict]:
        """
        åˆ›å»ºå€™é€‰èµ„æº / Create candidate resource

        Args:
            package: åŒ…å / Package name
            url: GitHub URL
            usage_count: ä½¿ç”¨æ¬¡æ•° / Usage count

        Returns:
            å€™é€‰èµ„æºæˆ– None / Candidate resource or None
        """
        # ç”Ÿæˆèµ„æº ID
        category_id = 'ecosystem'  # ä¾èµ–é€šå¸¸å½’ç±»ä¸ºç”Ÿæ€ç³»ç»Ÿ
        prefix = self.categories_prefix.get(category_id, 'eco')
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        resource_id = f"{prefix}-{url_hash}"

        today = datetime.now().strftime('%Y/%m/%d')

        # è·å–ä»“åº“ä¿¡æ¯
        github_info = extract_github_info(url)
        author = ''
        author_url = ''

        if github_info:
            owner, repo = github_info
            author = owner
            author_url = f"https://github.com/{owner}"

        return {
            'ID': resource_id,
            'DisplayName': package,
            'DisplayName_ZH': package,
            'Category': category_id,
            'SubCategory': 'general',
            'PrimaryLink': url,
            'SecondaryLink': '',
            'Author': author,
            'AuthorProfile': author_url,
            'IsActive': 'TRUE',
            'DateAdded': today,
            'LastModified': today,
            'LastChecked': today,
            'License': '',
            'Description': f"A dependency commonly used in Claude Code ecosystem (used by {usage_count} resources)",
            'Description_ZH': f"Claude Code ç”Ÿæ€ç³»ç»Ÿä¸­å¸¸ç”¨çš„ä¾èµ–åŒ…ï¼ˆè¢« {usage_count} ä¸ªèµ„æºä½¿ç”¨ï¼‰",
            'Tags_ZH': '',
            'IsPinned': 'FALSE',
            'Section': 'community',
            # å…ƒæ•°æ®
            '_source': 'dependency-analysis',
            '_discovered_at': datetime.now().isoformat(),
            '_status': 'pending',
            '_usage_count': usage_count,
            '_used_by': self.dependency_sources.get(package, [])[:5],
        }


def add_to_pending(resources: List[dict]) -> int:
    """æ·»åŠ èµ„æºåˆ°å¾…å®¡æ ¸é˜Ÿåˆ— / Add resources to pending queue"""
    pending_file = PROJECT_ROOT / 'candidates' / 'pending_resources.json'

    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {
            "_comment": "å€™é€‰èµ„æºé˜Ÿåˆ— - å¾…å®¡æ ¸çš„èµ„æº",
            "_schema_version": "1.0",
            "resources": []
        }

    existing_urls = {r.get('PrimaryLink', '').lower() for r in data['resources']}
    added_count = 0

    for res in resources:
        url = res.get('PrimaryLink', '').lower()
        if url and url not in existing_urls:
            data['resources'].append(res)
            existing_urls.add(url)
            added_count += 1

    with open(pending_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return added_count


def main():
    """ä¸»å‡½æ•° / Main function"""
    parser = argparse.ArgumentParser(description='Dependency Analysis')
    parser.add_argument('--analyze', action='store_true', help='Analyze dependencies')
    parser.add_argument('--discover', action='store_true', help='Discover related packages')
    parser.add_argument('--dry-run', action='store_true', help='Do not modify files')
    parser.add_argument('--min-count', type=int, default=2, help='Minimum usage count')
    args = parser.parse_args()

    print("ğŸ“¦ ä¾èµ–åˆ†æ / Dependency Analysis")
    print("=" * 50)

    analyzer = DependencyAnalyzer()

    if args.analyze:
        print("\nğŸ”¬ åˆ†æèµ„æºä¾èµ–...")
        results = analyzer.analyze_all_resources()

        print("\nğŸ“Š åˆ†æå®Œæˆ")
        print(f"   åˆ†æäº† {len(results)} ä¸ªä»“åº“")
        print(f"   å‘ç° {len(analyzer.dependency_counts)} ä¸ªç›¸å…³ä¾èµ–")

        # æ˜¾ç¤ºå¸¸ç”¨ä¾èµ–
        popular = analyzer.get_popular_dependencies(args.min_count)
        if popular:
            print(f"\nğŸ“ˆ å¸¸ç”¨ä¾èµ– (å‡ºç° >= {args.min_count} æ¬¡):")
            for dep, count in popular:
                print(f"   - {dep}: {count} æ¬¡")

    if args.discover:
        if not analyzer.dependency_counts:
            print("\nâš ï¸ è¯·å…ˆè¿è¡Œ --analyze åˆ†æä¾èµ–")
            return 1

        print("\nğŸ” å‘ç°ç›¸å…³åŒ…...")
        candidates = analyzer.discover_related_packages()

        if candidates:
            print(f"\nğŸ“¦ å‘ç° {len(candidates)} ä¸ªå€™é€‰èµ„æº:")
            for c in candidates:
                print(f"   - {c['DisplayName']}: {c['PrimaryLink']}")

            if not args.dry_run:
                added = add_to_pending(candidates)
                print(f"\nâœ… å·²æ·»åŠ  {added} ä¸ªèµ„æºåˆ°å€™é€‰é˜Ÿåˆ—")
            else:
                print("\n[Dry Run] è·³è¿‡ä¿å­˜")
        else:
            print("\nğŸ“­ æœªå‘ç°æ–°çš„ç›¸å…³åŒ…")

        return 0

    # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
    parser.print_help()
    return 0


if __name__ == '__main__':
    sys.exit(main())
