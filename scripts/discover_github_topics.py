#!/usr/bin/env python3
"""
GitHub Topics èµ„æºå‘ç°è„šæœ¬ / GitHub Topics Resource Discovery Script

é€šè¿‡ GitHub Topics å’Œå…³é”®è¯æœç´¢å‘ç°ä¸ Claude Code ç›¸å…³çš„æ–°é¡¹ç›®ã€‚
Discovers new Claude Code related projects via GitHub Topics and keyword search.

ç”¨æ³• / Usage:
    python scripts/discover_github_topics.py [--dry-run] [--limit N]
"""

import argparse
import csv
import hashlib
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import requests
import yaml

# é¡¹ç›®æ ¹ç›®å½• / Project root
PROJECT_ROOT = Path(__file__).parent.parent


def load_config() -> dict:
    """åŠ è½½å‘ç°é…ç½® / Load discovery configuration"""
    config_file = PROJECT_ROOT / "config" / "discovery.yaml"
    with open(config_file, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_categories() -> dict:
    """åŠ è½½åˆ†ç±»å®šä¹‰ / Load category definitions"""
    categories_file = PROJECT_ROOT / "templates" / "categories.yaml"
    with open(categories_file, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    return {cat["id"]: cat["prefix"] for cat in data["categories"]}


def load_existing_urls() -> Set[str]:
    """
    åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„èµ„æº URLï¼ˆCSV + pending + rejectedï¼‰
    Load all existing resource URLs (CSV + pending + rejected)
    """
    urls = set()

    # ä» CSV åŠ è½½
    csv_file = PROJECT_ROOT / "THE_RESOURCES_TABLE.csv"
    if csv_file.exists():
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get("PrimaryLink", "").strip().rstrip("/").lower()
                if url:
                    urls.add(url)

    # ä» pending åŠ è½½
    pending_file = PROJECT_ROOT / "candidates" / "pending_resources.json"
    if pending_file.exists():
        with open(pending_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            for res in data.get("resources", []):
                url = res.get("PrimaryLink", "").strip().rstrip("/").lower()
                if url:
                    urls.add(url)

    # ä» rejected åŠ è½½
    rejected_file = PROJECT_ROOT / "candidates" / "rejected_resources.json"
    if rejected_file.exists():
        with open(rejected_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            for res in data.get("resources", []):
                url = res.get("PrimaryLink", "").strip().rstrip("/").lower()
                if url:
                    urls.add(url)

    return urls


def load_discovery_log() -> dict:
    """åŠ è½½å‘ç°æ—¥å¿— / Load discovery log"""
    log_file = PROJECT_ROOT / "candidates" / "discovery_log.json"
    if log_file.exists():
        with open(log_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return {
        "_comment": "èµ„æºå‘ç°æ—¥å¿— / Resource discovery log",
        "last_run": None,
        "discovered_repos": [],
        "stats": {"total_discovered": 0, "total_added": 0, "total_skipped": 0},
    }


def save_discovery_log(log: dict):
    """ä¿å­˜å‘ç°æ—¥å¿— / Save discovery log"""
    log_file = PROJECT_ROOT / "candidates" / "discovery_log.json"
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


def github_search(query: str, token: Optional[str] = None, max_results: int = 50) -> List[dict]:
    """
    æ‰§è¡Œ GitHub æœç´¢ / Execute GitHub search

    Args:
        query: æœç´¢æŸ¥è¯¢ / Search query
        token: GitHub tokenï¼ˆå¯é€‰ï¼‰/ GitHub token (optional)
        max_results: æœ€å¤§ç»“æœæ•° / Maximum results

    Returns:
        ä»“åº“åˆ—è¡¨ / List of repositories
    """
    headers = {"Accept": "application/vnd.github+json", "X-GitHub-Api-Version": "2022-11-28"}
    if token:
        headers["Authorization"] = f"Bearer {token}"

    repos = []
    page = 1
    per_page = min(100, max_results)

    while len(repos) < max_results:
        url = "https://api.github.com/search/repositories"
        params = {"q": query, "sort": "stars", "order": "desc", "per_page": per_page, "page": page}

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            items = data.get("items", [])
            if not items:
                break

            repos.extend(items)
            page += 1

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šç»“æœ
            if len(items) < per_page:
                break

        except requests.exceptions.RequestException as e:
            print(f"   âš ï¸ GitHub API è¯·æ±‚å¤±è´¥: {e}")
            break

    return repos[:max_results]


def search_by_topic(topic: str, config: dict, token: Optional[str] = None) -> List[dict]:
    """
    æŒ‰ Topic æœç´¢ä»“åº“ / Search repositories by topic

    Args:
        topic: GitHub topic
        config: é…ç½® / Configuration
        token: GitHub token

    Returns:
        ä»“åº“åˆ—è¡¨ / List of repositories
    """
    github_config = config["github"]
    min_stars = github_config.get("min_stars", 3)
    max_results = github_config.get("max_results_per_query", 50)

    query = f"topic:{topic} stars:>={min_stars}"
    return github_search(query, token, max_results)


def search_by_keyword(keyword: str, config: dict, token: Optional[str] = None) -> List[dict]:
    """
    æŒ‰å…³é”®è¯æœç´¢ä»“åº“ / Search repositories by keyword

    Args:
        keyword: æœç´¢å…³é”®è¯ / Search keyword
        config: é…ç½® / Configuration
        token: GitHub token

    Returns:
        ä»“åº“åˆ—è¡¨ / List of repositories
    """
    github_config = config["github"]
    min_stars = github_config.get("min_stars", 3)
    max_results = github_config.get("max_results_per_query", 50)

    query = f"{keyword} in:name,description,readme stars:>={min_stars}"
    return github_search(query, token, max_results)


def filter_repo(repo: dict, config: dict, existing_urls: Set[str]) -> Tuple[bool, str]:
    """
    è¿‡æ»¤ä»“åº“ / Filter repository

    Args:
        repo: ä»“åº“ä¿¡æ¯ / Repository info
        config: é…ç½® / Configuration
        existing_urls: å·²å­˜åœ¨çš„ URL / Existing URLs

    Returns:
        (æ˜¯å¦é€šè¿‡, åŸå› ) / (passed, reason)
    """
    github_config = config["github"]

    # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
    full_name = repo.get("full_name", "").lower()
    excluded_repos = [r.lower() for r in github_config.get("excluded_repos", [])]
    if full_name in excluded_repos:
        return False, "åœ¨æ’é™¤åˆ—è¡¨ä¸­ / In exclusion list"

    # æ£€æŸ¥æ‰€æœ‰è€…æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
    owner = repo.get("owner", {}).get("login", "").lower()
    excluded_owners = [o.lower() for o in github_config.get("excluded_owners", [])]
    if owner in excluded_owners:
        return False, "æ‰€æœ‰è€…åœ¨æ’é™¤åˆ—è¡¨ä¸­ / Owner in exclusion list"

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    html_url = repo.get("html_url", "").strip().rstrip("/").lower()
    if html_url in existing_urls:
        return False, "å·²å­˜åœ¨ / Already exists"

    # æ£€æŸ¥ Star æ•°
    stars = repo.get("stargazers_count", 0)
    min_stars = github_config.get("min_stars", 3)
    if stars < min_stars:
        return False, f"Star æ•°ä¸è¶³ ({stars} < {min_stars})"

    # æ£€æŸ¥é¡¹ç›®å¹´é¾„
    created_at = repo.get("created_at")
    if created_at:
        created_date = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
        now = datetime.now(created_date.tzinfo)
        age_days = (now - created_date).days

        min_age = github_config.get("min_age_days", 7)
        if age_days < min_age:
            return False, f"é¡¹ç›®å¤ªæ–° ({age_days} < {min_age} å¤©)"

        max_age = github_config.get("max_age_days", 365)
        if age_days > max_age:
            return False, f"é¡¹ç›®å¤ªæ—§ ({age_days} > {max_age} å¤©)"

    # æ£€æŸ¥æ˜¯å¦è¢«å½’æ¡£
    if repo.get("archived", False):
        return False, "å·²å½’æ¡£ / Archived"

    return True, "é€šè¿‡ / Passed"


def calculate_relevance_score(repo: dict, config: dict) -> int:
    """
    è®¡ç®—ç›¸å…³æ€§è¯„åˆ† / Calculate relevance score

    Args:
        repo: ä»“åº“ä¿¡æ¯ / Repository info
        config: é…ç½® / Configuration

    Returns:
        ç›¸å…³æ€§è¯„åˆ† (0-100) / Relevance score (0-100)
    """
    score = 0
    indicators = config["github"].get("relevance_indicators", {})

    # æ£€æŸ¥é«˜ç›¸å…³æ€§æŒ‡æ ‡
    high_indicators = indicators.get("high", [])
    description = (repo.get("description") or "").lower()
    name = repo.get("name", "").lower()
    topics = [t.lower() for t in repo.get("topics", [])]

    for indicator in high_indicators:
        indicator_lower = indicator.lower()
        if indicator_lower in name or indicator_lower in description:
            score += 30
        if indicator_lower in topics:
            score += 20

    # æ£€æŸ¥ä¸­ç­‰ç›¸å…³æ€§æŒ‡æ ‡
    medium_indicators = indicators.get("medium", [])
    for indicator in medium_indicators:
        indicator_lower = indicator.lower()
        if indicator_lower in name:
            score += 15
        if indicator_lower in description:
            score += 10
        if indicator_lower in topics:
            score += 10

    # åŸºäº Star æ•°åŠ åˆ†
    stars = repo.get("stargazers_count", 0)
    if stars >= 100:
        score += 15
    elif stars >= 50:
        score += 10
    elif stars >= 20:
        score += 5

    # åŸºäºæœ€è¿‘æ›´æ–°åŠ åˆ†
    pushed_at = repo.get("pushed_at")
    if pushed_at:
        pushed_date = datetime.fromisoformat(pushed_at.replace("Z", "+00:00"))
        now = datetime.now(pushed_date.tzinfo)
        days_since_push = (now - pushed_date).days

        if days_since_push <= 7:
            score += 10
        elif days_since_push <= 30:
            score += 5

    return min(100, score)


def infer_category(repo: dict, config: dict) -> str:
    """
    æ¨æ–­èµ„æºåˆ†ç±» / Infer resource category

    Args:
        repo: ä»“åº“ä¿¡æ¯ / Repository info
        config: é…ç½® / Configuration

    Returns:
        åˆ†ç±» ID / Category ID
    """
    inference = config.get("category_inference", {})
    topic_mapping = inference.get("topic_mapping", {})
    default_category = inference.get("default_category", "ecosystem")

    # åŸºäº Topics æ¨æ–­
    topics = repo.get("topics", [])
    for topic in topics:
        if topic in topic_mapping:
            return topic_mapping[topic]

    # åŸºäºåç§°å’Œæè¿°æ¨æ–­
    name = repo.get("name", "").lower()
    description = (repo.get("description") or "").lower()
    combined = f"{name} {description}"

    if "mcp" in combined or "model-context-protocol" in combined:
        return "mcp-servers"
    if "hook" in combined:
        return "hooks"
    if "slash" in combined or "command" in combined:
        return "slash-commands"
    if "workflow" in combined:
        return "workflows"
    if "tool" in combined or "extension" in combined or "plugin" in combined:
        return "tooling"
    if "skill" in combined:
        return "skills"

    return default_category


def generate_resource_id(category_id: str, url: str, categories_prefix: dict) -> str:
    """ç”Ÿæˆèµ„æº ID / Generate resource ID"""
    prefix = categories_prefix.get(category_id, "res")
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    return f"{prefix}-{url_hash}"


def create_candidate_from_repo(repo: dict, config: dict, categories_prefix: dict, relevance_score: int) -> dict:
    """
    ä»ä»“åº“ä¿¡æ¯åˆ›å»ºå€™é€‰èµ„æº / Create candidate resource from repository info

    Args:
        repo: ä»“åº“ä¿¡æ¯ / Repository info
        config: é…ç½® / Configuration
        categories_prefix: åˆ†ç±»å‰ç¼€æ˜ å°„ / Category prefix mapping
        relevance_score: ç›¸å…³æ€§è¯„åˆ† / Relevance score

    Returns:
        å€™é€‰èµ„æºå­—å…¸ / Candidate resource dict
    """
    url = repo.get("html_url", "")
    category_id = infer_category(repo, config)
    resource_id = generate_resource_id(category_id, url, categories_prefix)

    today = datetime.now().strftime("%Y/%m/%d")
    owner = repo.get("owner", {})

    description = repo.get("description") or ""
    # æˆªæ–­è¿‡é•¿çš„æè¿°
    if len(description) > 200:
        description = description[:197] + "..."

    return {
        "ID": resource_id,
        "DisplayName": repo.get("name", ""),
        "DisplayName_ZH": repo.get("name", ""),  # éœ€è¦äººå·¥ç¿»è¯‘
        "Category": category_id,
        "SubCategory": "general",
        "PrimaryLink": url,
        "SecondaryLink": repo.get("homepage", "") or "",
        "Author": owner.get("login", ""),
        "AuthorProfile": owner.get("html_url", ""),
        "IsActive": "TRUE",
        "DateAdded": today,
        "LastModified": today,
        "LastChecked": today,
        "License": repo.get("license", {}).get("spdx_id", "") if repo.get("license") else "",
        "Description": description,
        "Description_ZH": "",  # éœ€è¦äººå·¥ç¿»è¯‘
        "Tags_ZH": "",
        "IsPinned": "FALSE",
        "Section": "community",
        # å…ƒæ•°æ®
        "_source": "github-discovery",
        "_discovered_at": datetime.now().isoformat(),
        "_status": "pending",
        "_relevance_score": relevance_score,
        "_stars": repo.get("stargazers_count", 0),
        "_language": repo.get("language", ""),
        "_topics": repo.get("topics", []),
    }


def add_to_pending(resource: dict, pending_file: Path) -> bool:
    """æ·»åŠ èµ„æºåˆ°å¾…å®¡æ ¸é˜Ÿåˆ— / Add resource to pending queue"""
    if pending_file.exists():
        with open(pending_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = {
            "_comment": "å€™é€‰èµ„æºé˜Ÿåˆ— - å¾…å®¡æ ¸çš„èµ„æº / Candidate resource queue - resources pending review",
            "_schema_version": "1.0",
            "resources": [],
        }

    data["resources"].append(resource)

    with open(pending_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return True


def main():
    """ä¸»å‡½æ•° / Main function"""
    parser = argparse.ArgumentParser(description="Discover GitHub resources")
    parser.add_argument("--dry-run", action="store_true", help="Do not modify files")
    parser.add_argument("--limit", type=int, default=10, help="Maximum resources to add")
    parser.add_argument("--topics-only", action="store_true", help="Only search by topics")
    parser.add_argument("--keywords-only", action="store_true", help="Only search by keywords")
    args = parser.parse_args()

    print("ğŸ” GitHub èµ„æºå‘ç° / GitHub Resource Discovery")
    print("=" * 50)

    # è·å– GitHub token
    token = os.environ.get("GITHUB_TOKEN")
    if not token:
        print("âš ï¸  æœªè®¾ç½® GITHUB_TOKENï¼ŒAPI é€Ÿç‡é™åˆ¶è¾ƒä½")
        print("âš ï¸  GITHUB_TOKEN not set, API rate limit is lower")

    # åŠ è½½é…ç½®
    print("\nğŸ“‚ åŠ è½½é…ç½®...")
    config = load_config()
    categories_prefix = load_categories()
    existing_urls = load_existing_urls()
    discovery_log = load_discovery_log()

    print(f"   å·²æœ‰èµ„æºæ•°: {len(existing_urls)}")

    github_config = config["github"]
    all_repos = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼Œkey ä¸º full_name

    # æŒ‰ Topics æœç´¢
    if not args.keywords_only:
        topics = github_config.get("topics", [])
        print(f"\nğŸ·ï¸  æœç´¢ Topics ({len(topics)} ä¸ª)...")

        for topic in topics:
            print(f"   æœç´¢ topic:{topic}...")
            repos = search_by_topic(topic, config, token)
            for repo in repos:
                full_name = repo.get("full_name", "")
                if full_name and full_name not in all_repos:
                    all_repos[full_name] = repo
            print(f"      æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“ï¼Œç´¯è®¡ {len(all_repos)} ä¸ª")

    # æŒ‰å…³é”®è¯æœç´¢
    if not args.topics_only:
        queries = github_config.get("search_queries", [])
        print(f"\nğŸ” æœç´¢å…³é”®è¯ ({len(queries)} ä¸ª)...")

        for query in queries:
            print(f'   æœç´¢ "{query}"...')
            repos = search_by_keyword(query, config, token)
            for repo in repos:
                full_name = repo.get("full_name", "")
                if full_name and full_name not in all_repos:
                    all_repos[full_name] = repo
            print(f"      æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“ï¼Œç´¯è®¡ {len(all_repos)} ä¸ª")

    print(f"\nğŸ“Š å…±å‘ç° {len(all_repos)} ä¸ªå”¯ä¸€ä»“åº“")

    # è¿‡æ»¤å’Œè¯„åˆ†
    print("\nğŸ”¬ è¿‡æ»¤å’Œè¯„åˆ†...")
    candidates = []

    for full_name, repo in all_repos.items():
        passed, reason = filter_repo(repo, config, existing_urls)
        if not passed:
            continue

        score = calculate_relevance_score(repo, config)
        if score < 20:  # ç›¸å…³æ€§è¯„åˆ†è¿‡ä½
            continue

        candidates.append((repo, score))

    # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
    candidates.sort(key=lambda x: x[1], reverse=True)
    candidates = candidates[: args.limit]

    print(f"   ç¬¦åˆæ¡ä»¶çš„å€™é€‰: {len(candidates)} ä¸ª")

    if not candidates:
        print("\nğŸ“­ æ²¡æœ‰å‘ç°æ–°çš„å€™é€‰èµ„æº")
        return 0

    # åˆ›å»ºå€™é€‰èµ„æº
    print(f"\nğŸ“¦ åˆ›å»ºå€™é€‰èµ„æº (é™åˆ¶ {args.limit} ä¸ª)...")
    pending_file = PROJECT_ROOT / "candidates" / "pending_resources.json"
    added_count = 0

    for repo, score in candidates:
        resource = create_candidate_from_repo(repo, config, categories_prefix, score)

        print(f"\n   ğŸ“Œ {resource['DisplayName']}")
        print(f"      URL: {resource['PrimaryLink']}")
        print(f"      åˆ†ç±»: {resource['Category']}")
        print(f"      Stars: {resource['_stars']} â­")
        print(f"      ç›¸å…³æ€§: {score}/100")

        if args.dry_run:
            print("      [Dry Run] è·³è¿‡æ·»åŠ ")
        else:
            add_to_pending(resource, pending_file)
            added_count += 1
            print("      âœ… å·²æ·»åŠ åˆ°å€™é€‰é˜Ÿåˆ—")

            # æ›´æ–°å‘ç°æ—¥å¿—
            discovery_log["discovered_repos"].append(
                {
                    "full_name": repo.get("full_name"),
                    "url": resource["PrimaryLink"],
                    "discovered_at": datetime.now().isoformat(),
                    "relevance_score": score,
                }
            )

    # ä¿å­˜å‘ç°æ—¥å¿—
    if not args.dry_run:
        discovery_log["last_run"] = datetime.now().isoformat()
        discovery_log["stats"]["total_discovered"] += len(candidates)
        discovery_log["stats"]["total_added"] += added_count
        save_discovery_log(discovery_log)

    print(f"\nâœ… å®Œæˆï¼æ·»åŠ äº† {added_count} ä¸ªæ–°å€™é€‰èµ„æº")

    # è¾“å‡ºä¾› GitHub Actions ä½¿ç”¨
    print(f"::set-output name=discovered_count::{len(candidates)}")
    print(f"::set-output name=added_count::{added_count}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
