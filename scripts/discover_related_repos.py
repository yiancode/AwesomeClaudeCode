#!/usr/bin/env python3
"""
å…³è”é¡¹ç›®å‘ç°è„šæœ¬ / Related Repository Discovery Script

ä»ç°æœ‰èµ„æºå‘ç°å…³è”é¡¹ç›®ï¼ˆä¾èµ–ã€Forkã€ç›¸ä¼¼é¡¹ç›®ç­‰ï¼‰ã€‚
Discovers related repositories from existing resources (dependencies, forks, similar projects, etc.).

ç”¨æ³• / Usage:
    python scripts/discover_related_repos.py [--dry-run] [--limit N] [--type TYPE]
"""

import argparse
import csv
import hashlib
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import requests
import yaml

# é¡¹ç›®æ ¹ç›®å½• / Project root
PROJECT_ROOT = Path(__file__).parent.parent


def load_config() -> dict:
    """åŠ è½½å‘ç°é…ç½® / Load discovery configuration"""
    config_file = PROJECT_ROOT / "config" / "discovery.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_categories() -> dict:
    """åŠ è½½åˆ†ç±»å®šä¹‰ / Load category definitions"""
    categories_file = PROJECT_ROOT / "templates" / "categories.yaml"
    with open(categories_file, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    return {cat['id']: cat['prefix'] for cat in data['categories']}


def load_existing_resources() -> List[dict]:
    """åŠ è½½ç°æœ‰èµ„æº / Load existing resources"""
    resources = []
    csv_file = PROJECT_ROOT / 'THE_RESOURCES_TABLE.csv'

    if csv_file.exists():
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                resources.append(row)

    return resources


def load_existing_urls() -> Set[str]:
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„ URL / Load all existing URLs"""
    urls = set()

    # ä» CSV åŠ è½½
    csv_file = PROJECT_ROOT / 'THE_RESOURCES_TABLE.csv'
    if csv_file.exists():
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get('PrimaryLink', '').strip().rstrip('/').lower()
                if url:
                    urls.add(url)

    # ä» pending åŠ è½½
    pending_file = PROJECT_ROOT / 'candidates' / 'pending_resources.json'
    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for res in data.get('resources', []):
                url = res.get('PrimaryLink', '').strip().rstrip('/').lower()
                if url:
                    urls.add(url)

    # ä» rejected åŠ è½½
    rejected_file = PROJECT_ROOT / 'candidates' / 'rejected_resources.json'
    if rejected_file.exists():
        with open(rejected_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for res in data.get('resources', []):
                url = res.get('PrimaryLink', '').strip().rstrip('/').lower()
                if url:
                    urls.add(url)

    return urls


def extract_github_info(url: str) -> Optional[Tuple[str, str]]:
    """
    ä» URL æå– GitHub owner/repo ä¿¡æ¯
    Extract GitHub owner/repo from URL

    Returns: (owner, repo) or None
    """
    if 'github.com' not in url:
        return None

    # è§£æ URL
    parts = url.rstrip('/').split('/')
    try:
        github_index = next(i for i, p in enumerate(parts) if 'github.com' in p)
        if len(parts) > github_index + 2:
            owner = parts[github_index + 1]
            repo = parts[github_index + 2]
            # ç§»é™¤å¯èƒ½çš„ .git åç¼€
            repo = repo.replace('.git', '')
            return (owner, repo)
    except (StopIteration, IndexError):
        pass

    return None


def get_repo_info(owner: str, repo: str, token: Optional[str] = None) -> Optional[dict]:
    """è·å–ä»“åº“ä¿¡æ¯ / Get repository info"""
    headers = {
        'Accept': 'application/vnd.github+json',
        'X-GitHub-Api-Version': '2022-11-28'
    }
    if token:
        headers['Authorization'] = f'Bearer {token}'

    url = f"https://api.github.com/repos/{owner}/{repo}"

    try:
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            return response.json()
    except requests.exceptions.RequestException:
        pass

    return None


def get_forks(owner: str, repo: str, token: Optional[str] = None, limit: int = 30) -> List[dict]:
    """
    è·å–ä»“åº“çš„ Fork åˆ—è¡¨ / Get repository forks

    Args:
        owner: ä»“åº“æ‰€æœ‰è€… / Repository owner
        repo: ä»“åº“å / Repository name
        token: GitHub token
        limit: æœ€å¤§æ•°é‡ / Maximum count

    Returns:
        Fork åˆ—è¡¨ / List of forks
    """
    headers = {
        'Accept': 'application/vnd.github+json',
        'X-GitHub-Api-Version': '2022-11-28'
    }
    if token:
        headers['Authorization'] = f'Bearer {token}'

    url = f"https://api.github.com/repos/{owner}/{repo}/forks"
    params = {
        'sort': 'stargazers',
        'per_page': min(100, limit)
    }

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        if response.status_code == 200:
            return response.json()[:limit]
    except requests.exceptions.RequestException:
        pass

    return []


def get_stargazers_also_starred(
    owner: str,
    repo: str,
    token: Optional[str] = None,
    sample_size: int = 20
) -> List[dict]:
    """
    è·å– Star äº†è¯¥é¡¹ç›®çš„ç”¨æˆ·ä¹Ÿ Star çš„å…¶ä»–é¡¹ç›®ï¼ˆç›¸ä¼¼é¡¹ç›®å‘ç°ï¼‰
    Get other projects starred by users who starred this project (similar project discovery)

    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œåªé‡‡æ ·éƒ¨åˆ†ç”¨æˆ·
    This is a simplified implementation that only samples some users
    """
    headers = {
        'Accept': 'application/vnd.github+json',
        'X-GitHub-Api-Version': '2022-11-28'
    }
    if token:
        headers['Authorization'] = f'Bearer {token}'

    # è·å–éƒ¨åˆ† stargazers
    stargazers_url = f"https://api.github.com/repos/{owner}/{repo}/stargazers"
    params = {'per_page': sample_size}

    try:
        response = requests.get(stargazers_url, headers=headers, params=params, timeout=30)
        if response.status_code != 200:
            return []

        stargazers = response.json()
    except requests.exceptions.RequestException:
        return []

    # æ”¶é›†è¿™äº›ç”¨æˆ· Star çš„å…¶ä»–é¡¹ç›®
    related_repos = {}  # {full_name: repo_info}

    for user in stargazers[:10]:  # é™åˆ¶åªæŸ¥çœ‹å‰ 10 ä¸ªç”¨æˆ·
        username = user.get('login')
        if not username:
            continue

        starred_url = f"https://api.github.com/users/{username}/starred"
        params = {'per_page': 30}

        try:
            response = requests.get(starred_url, headers=headers, params=params, timeout=30)
            if response.status_code == 200:
                for starred_repo in response.json():
                    full_name = starred_repo.get('full_name', '')
                    # æ’é™¤åŸä»“åº“
                    if full_name.lower() != f"{owner}/{repo}".lower():
                        if full_name not in related_repos:
                            related_repos[full_name] = starred_repo
        except requests.exceptions.RequestException:
            continue

    return list(related_repos.values())


def filter_related_repo(
    repo: dict,
    config: dict,
    existing_urls: Set[str],
    is_fork: bool = False
) -> Tuple[bool, str]:
    """
    è¿‡æ»¤å…³è”ä»“åº“ / Filter related repository

    Returns: (passed, reason)
    """
    github_config = config['github']
    related_config = config.get('related_discovery', {})

    # æ£€æŸ¥ URL æ˜¯å¦å·²å­˜åœ¨
    html_url = repo.get('html_url', '').strip().rstrip('/').lower()
    if html_url in existing_urls:
        return False, "å·²å­˜åœ¨ / Already exists"

    # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
    full_name = repo.get('full_name', '').lower()
    excluded_repos = [r.lower() for r in github_config.get('excluded_repos', [])]
    if full_name in excluded_repos:
        return False, "åœ¨æ’é™¤åˆ—è¡¨ä¸­ / In exclusion list"

    # æ£€æŸ¥ Star æ•°
    stars = repo.get('stargazers_count', 0)
    if is_fork:
        min_stars = related_config.get('fork_min_stars', 10)
    else:
        min_stars = github_config.get('min_stars', 3)

    if stars < min_stars:
        return False, f"Star æ•°ä¸è¶³ ({stars} < {min_stars})"

    # æ£€æŸ¥æ˜¯å¦è¢«å½’æ¡£
    if repo.get('archived', False):
        return False, "å·²å½’æ¡£ / Archived"

    return True, "é€šè¿‡ / Passed"


def calculate_relevance_score(repo: dict, source_repo: dict, relation_type: str) -> int:
    """
    è®¡ç®—å…³è”ä»“åº“çš„ç›¸å…³æ€§è¯„åˆ†
    Calculate relevance score for related repository
    """
    score = 0

    # åŸºäºå…³ç³»ç±»å‹åŠ åˆ†
    if relation_type == 'fork':
        score += 20  # Fork åŸºç¡€åˆ†è¾ƒä½ï¼Œéœ€è¦æ›´å¤šå…¶ä»–æŒ‡æ ‡
    elif relation_type == 'similar':
        score += 30

    # åŸºäº Star æ•°åŠ åˆ†
    stars = repo.get('stargazers_count', 0)
    if stars >= 100:
        score += 25
    elif stars >= 50:
        score += 20
    elif stars >= 20:
        score += 15
    elif stars >= 10:
        score += 10

    # æ£€æŸ¥åç§°/æè¿°ä¸­æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
    name = repo.get('name', '').lower()
    description = (repo.get('description') or '').lower()
    combined = f"{name} {description}"

    keywords = ['claude', 'anthropic', 'mcp', 'llm', 'ai-assistant']
    for keyword in keywords:
        if keyword in combined:
            score += 15
            break

    # æœ€è¿‘æ›´æ–°åŠ åˆ†
    pushed_at = repo.get('pushed_at')
    if pushed_at:
        pushed_date = datetime.fromisoformat(pushed_at.replace('Z', '+00:00'))
        now = datetime.now(pushed_date.tzinfo)
        days_since_push = (now - pushed_date).days

        if days_since_push <= 30:
            score += 10
        elif days_since_push <= 90:
            score += 5

    return min(100, score)


def infer_category(repo: dict, source_category: str, config: dict) -> str:
    """æ¨æ–­åˆ†ç±»ï¼Œä¼˜å…ˆä½¿ç”¨æºä»“åº“çš„åˆ†ç±» / Infer category, prefer source repo's category"""
    # å¦‚æœæ˜¯ Forkï¼Œç»§æ‰¿æºåˆ†ç±»
    if repo.get('fork', False):
        return source_category

    # åŸºäºåç§°å’Œæè¿°æ¨æ–­
    name = repo.get('name', '').lower()
    description = (repo.get('description') or '').lower()
    combined = f"{name} {description}"

    if 'mcp' in combined or 'model-context-protocol' in combined:
        return 'mcp-servers'
    if 'hook' in combined:
        return 'hooks'
    if 'workflow' in combined:
        return 'workflows'
    if 'tool' in combined or 'extension' in combined:
        return 'tooling'

    # é»˜è®¤ä½¿ç”¨æºåˆ†ç±»æˆ– ecosystem
    return source_category or 'ecosystem'


def generate_resource_id(category_id: str, url: str, categories_prefix: dict) -> str:
    """ç”Ÿæˆèµ„æº ID / Generate resource ID"""
    prefix = categories_prefix.get(category_id, 'res')
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    return f"{prefix}-{url_hash}"


def create_candidate_from_repo(
    repo: dict,
    source_resource: dict,
    relation_type: str,
    config: dict,
    categories_prefix: dict,
    relevance_score: int
) -> dict:
    """åˆ›å»ºå€™é€‰èµ„æº / Create candidate resource"""
    url = repo.get('html_url', '')
    source_category = source_resource.get('Category', 'ecosystem')
    category_id = infer_category(repo, source_category, config)
    resource_id = generate_resource_id(category_id, url, categories_prefix)

    today = datetime.now().strftime('%Y/%m/%d')
    owner = repo.get('owner', {})

    description = repo.get('description') or ''
    if len(description) > 200:
        description = description[:197] + '...'

    return {
        'ID': resource_id,
        'DisplayName': repo.get('name', ''),
        'DisplayName_ZH': repo.get('name', ''),
        'Category': category_id,
        'SubCategory': 'general',
        'PrimaryLink': url,
        'SecondaryLink': repo.get('homepage', '') or '',
        'Author': owner.get('login', ''),
        'AuthorProfile': owner.get('html_url', ''),
        'IsActive': 'TRUE',
        'DateAdded': today,
        'LastModified': today,
        'LastChecked': today,
        'License': repo.get('license', {}).get('spdx_id', '') if repo.get('license') else '',
        'Description': description,
        'Description_ZH': '',
        'Tags_ZH': '',
        'IsPinned': 'FALSE',
        'Section': 'community',
        # å…ƒæ•°æ®
        '_source': 'related-discovery',
        '_source_repo': source_resource.get('PrimaryLink', ''),
        '_relation_type': relation_type,
        '_discovered_at': datetime.now().isoformat(),
        '_status': 'pending',
        '_relevance_score': relevance_score,
        '_stars': repo.get('stargazers_count', 0),
        '_language': repo.get('language', ''),
    }


def add_to_pending(resource: dict, pending_file: Path) -> bool:
    """æ·»åŠ åˆ°å¾…å®¡æ ¸é˜Ÿåˆ— / Add to pending queue"""
    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {
            "_comment": "å€™é€‰èµ„æºé˜Ÿåˆ— - å¾…å®¡æ ¸çš„èµ„æº",
            "_schema_version": "1.0",
            "resources": []
        }

    data['resources'].append(resource)

    with open(pending_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return True


def main():
    """ä¸»å‡½æ•° / Main function"""
    parser = argparse.ArgumentParser(description='Discover related repositories')
    parser.add_argument('--dry-run', action='store_true', help='Do not modify files')
    parser.add_argument('--limit', type=int, default=10, help='Maximum resources to add')
    parser.add_argument('--type', choices=['forks', 'similar', 'all'], default='all',
                        help='Type of relation to discover')
    args = parser.parse_args()

    print("ğŸ”— å…³è”é¡¹ç›®å‘ç° / Related Repository Discovery")
    print("=" * 50)

    # è·å– GitHub token
    token = os.environ.get('GITHUB_TOKEN')
    if not token:
        print("âš ï¸  æœªè®¾ç½® GITHUB_TOKENï¼ŒAPI é€Ÿç‡é™åˆ¶è¾ƒä½")

    # åŠ è½½é…ç½®å’Œæ•°æ®
    print("\nğŸ“‚ åŠ è½½é…ç½®å’Œæ•°æ®...")
    config = load_config()
    categories_prefix = load_categories()
    existing_resources = load_existing_resources()
    existing_urls = load_existing_urls()

    # è¿‡æ»¤å‡º GitHub èµ„æº
    github_resources = []
    for res in existing_resources:
        url = res.get('PrimaryLink', '')
        github_info = extract_github_info(url)
        if github_info:
            github_resources.append((res, github_info))

    print(f"   ç°æœ‰ GitHub èµ„æº: {len(github_resources)} ä¸ª")

    if not github_resources:
        print("\nğŸ“­ æ²¡æœ‰å¯åˆ†æçš„ GitHub èµ„æº")
        return 0

    # å‘ç°å…³è”é¡¹ç›®
    candidates = []
    pending_file = PROJECT_ROOT / 'candidates' / 'pending_resources.json'

    # å‘ç° Fork é¡¹ç›®
    if args.type in ['forks', 'all']:
        print("\nğŸ´ å‘ç° Fork é¡¹ç›®...")
        for resource, (owner, repo) in github_resources[:20]:  # é™åˆ¶åˆ†ææ•°é‡
            print(f"   åˆ†æ {owner}/{repo}...")

            forks = get_forks(owner, repo, token, limit=10)
            for fork in forks:
                passed, reason = filter_related_repo(fork, config, existing_urls, is_fork=True)
                if not passed:
                    continue

                score = calculate_relevance_score(fork, resource, 'fork')
                if score >= 30:  # ç›¸å…³æ€§é˜ˆå€¼
                    candidates.append((fork, resource, 'fork', score))

    # å‘ç°ç›¸ä¼¼é¡¹ç›®
    if args.type in ['similar', 'all']:
        print("\nğŸ”„ å‘ç°ç›¸ä¼¼é¡¹ç›®...")
        for resource, (owner, repo) in github_resources[:10]:  # ç›¸ä¼¼é¡¹ç›®å‘ç°æ›´è€—æ—¶ï¼Œé™åˆ¶æ›´å¤š
            print(f"   åˆ†æ {owner}/{repo} çš„ç›¸ä¼¼é¡¹ç›®...")

            similar = get_stargazers_also_starred(owner, repo, token, sample_size=10)
            for sim_repo in similar:
                passed, reason = filter_related_repo(sim_repo, config, existing_urls)
                if not passed:
                    continue

                score = calculate_relevance_score(sim_repo, resource, 'similar')
                if score >= 40:  # ç›¸ä¼¼é¡¹ç›®éœ€è¦æ›´é«˜çš„ç›¸å…³æ€§
                    candidates.append((sim_repo, resource, 'similar', score))

    # å»é‡å’Œæ’åº
    seen_urls = set()
    unique_candidates = []
    for repo, source, rel_type, score in candidates:
        url = repo.get('html_url', '').lower()
        if url not in seen_urls:
            seen_urls.add(url)
            unique_candidates.append((repo, source, rel_type, score))

    unique_candidates.sort(key=lambda x: x[3], reverse=True)
    unique_candidates = unique_candidates[:args.limit]

    print(f"\nğŸ“Š å‘ç° {len(unique_candidates)} ä¸ªå€™é€‰å…³è”é¡¹ç›®")

    if not unique_candidates:
        print("\nğŸ“­ æ²¡æœ‰å‘ç°æ–°çš„å…³è”é¡¹ç›®")
        return 0

    # åˆ›å»ºå€™é€‰èµ„æº
    print(f"\nğŸ“¦ åˆ›å»ºå€™é€‰èµ„æº...")
    added_count = 0

    for repo, source_resource, relation_type, score in unique_candidates:
        resource = create_candidate_from_repo(
            repo, source_resource, relation_type,
            config, categories_prefix, score
        )

        print(f"\n   ğŸ“Œ {resource['DisplayName']}")
        print(f"      URL: {resource['PrimaryLink']}")
        print(f"      å…³ç³»: {relation_type} (æ¥è‡ª {source_resource.get('DisplayName', 'unknown')})")
        print(f"      Stars: {resource['_stars']} â­")
        print(f"      ç›¸å…³æ€§: {score}/100")

        if args.dry_run:
            print("      [Dry Run] è·³è¿‡æ·»åŠ ")
        else:
            add_to_pending(resource, pending_file)
            added_count += 1
            print("      âœ… å·²æ·»åŠ åˆ°å€™é€‰é˜Ÿåˆ—")

    print(f"\nâœ… å®Œæˆï¼æ·»åŠ äº† {added_count} ä¸ªå…³è”é¡¹ç›®")

    # è¾“å‡ºä¾› GitHub Actions ä½¿ç”¨
    print(f"::set-output name=discovered_count::{len(unique_candidates)}")
    print(f"::set-output name=added_count::{added_count}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
