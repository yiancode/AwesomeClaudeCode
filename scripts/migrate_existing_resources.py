#!/usr/bin/env python3
"""
è¿ç§»ç°æœ‰èµ„æºåˆ° CSV æ ¼å¼ / Migrate existing resources to CSV format

æ­¤è„šæœ¬ä» AwesomeClaudeCode çš„ README.md ä¸­æå–èµ„æº
å¹¶ç”Ÿæˆç¬¦åˆæ–°æ¶æ„çš„ THE_RESOURCES_TABLE.csv æ–‡ä»¶

This script extracts resources from AwesomeClaudeCode's README.md
and generates THE_RESOURCES_TABLE.csv following the new architecture
"""

import csv
import hashlib
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse

import yaml


def load_categories(categories_file: Path) -> Dict:
    """åŠ è½½åˆ†ç±»å®šä¹‰ / Load category definitions"""
    with open(categories_file, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    # åˆ›å»ºåˆ†ç±»æŸ¥æ‰¾å­—å…¸ / Create category lookup dictionary
    categories = {}
    for cat in data["categories"]:
        categories[cat["id"]] = cat
        # æ·»åŠ ä¸­æ–‡åç§°ä½œä¸ºåˆ«å / Add Chinese name as alias
        if "name_zh" in cat:
            categories[cat["name_zh"]] = cat

    return categories


def generate_resource_id(name: str, prefix: str) -> str:
    """
    ç”Ÿæˆèµ„æºå”¯ä¸€ ID / Generate unique resource ID
    æ ¼å¼ / Format: prefix-hash8
    """
    hash_input = name.lower().strip()
    hash_value = hashlib.sha256(hash_input.encode("utf-8")).hexdigest()[:8]
    return f"{prefix}-{hash_value}"


def extract_link_and_text(line: str) -> Optional[Dict[str, str]]:
    """
    ä» Markdown è¡Œä¸­æå–é“¾æ¥å’Œæ–‡æœ¬
    Extract link and text from Markdown line

    æ”¯æŒæ ¼å¼ / Supported formats:
    - [Name](url) - description
    - [Name](url) ğŸ“Œ å®˜æ–¹ - description
    - **[Name](url)** - description
    """
    # åŒ¹é… Markdown é“¾æ¥æ ¼å¼
    # Match Markdown link format
    link_pattern = r"\[([^\]]+)\]\(([^\)]+)\)"
    match = re.search(link_pattern, line)

    if not match:
        return None

    name = match.group(1).strip()
    url = match.group(2).strip()

    # æå–æè¿°ï¼ˆé“¾æ¥åçš„æ–‡æœ¬ï¼‰
    # Extract description (text after link)
    description_start = match.end()
    remaining = line[description_start:].strip()

    # ç§»é™¤å¯èƒ½çš„å¾½ç« å’Œåˆ†éš”ç¬¦
    # Remove possible badges and separators
    remaining = re.sub(r"^[ğŸ“ŒğŸ”¥â­]*\s*", "", remaining)
    remaining = re.sub(r"^\s*å®˜æ–¹\s*", "", remaining)
    remaining = re.sub(r"^\s*-\s*", "", remaining)

    description = remaining.strip()

    # æ£€æŸ¥æ˜¯å¦ä¸ºå®˜æ–¹èµ„æºï¼ˆæœ‰ ğŸ“Œ æˆ– "å®˜æ–¹" æ ‡è®°ï¼‰
    # Check if official resource (has ğŸ“Œ or "å®˜æ–¹" marker)
    is_pinned = "ğŸ“Œ" in line or "å®˜æ–¹" in line

    return {"name": name, "url": url, "description": description, "is_pinned": is_pinned}


def parse_readme(readme_file: Path, categories: Dict) -> List[Dict]:
    """
    è§£æ README.md å¹¶æå–èµ„æº
    Parse README.md and extract resources
    """
    with open(readme_file, "r", encoding="utf-8") as f:
        content = f.read()

    resources = []
    current_category = None
    current_section = None

    # åˆ†ç±»æ˜ å°„ï¼šä¸­æ–‡æ ‡é¢˜ -> category ID
    # Category mapping: Chinese title -> category ID
    category_mapping = {
        "å®˜æ–¹èµ„æº": "official-resources",
        "Anthropic æ–‡æ¡£ä¸ API": "official-resources",
        "Anthropic SDK åº“": "official-resources",
        "Anthropic æ•™ç¨‹ä¸ç¤ºä¾‹": "official-resources",
        "å®‰è£…ä¸é…ç½®": "workflows",
        "å…¥é—¨æ•™ç¨‹": "workflows",
        "é«˜çº§åŠŸèƒ½": "workflows",
        "æœ€ä½³å®è·µ": "workflows",
        "æ¡ˆä¾‹ç ”ç©¶": "case-studies",
        "ç¬¬ä¸‰æ–¹å·¥å…·": "tooling",
        "ç”Ÿæ€ç³»ç»Ÿ": "ecosystem",
        "MCP æœåŠ¡å™¨": "mcp-servers",
        "å¼€æºé¡¹ç›®": "open-source-projects",
    }

    # æŒ‰è¡Œå¤„ç†
    # Process line by line
    lines = content.split("\n")

    for line in lines:
        # æ£€æµ‹äºŒçº§æ ‡é¢˜ï¼ˆåˆ†ç±»ï¼‰
        # Detect level-2 heading (category)
        if line.startswith("## "):
            section_title = line[3:].strip()
            # ç§»é™¤ emoji
            section_title = re.sub(r"^[^\w\s]+\s*", "", section_title)

            # æŸ¥æ‰¾å¯¹åº”çš„åˆ†ç±» ID
            current_category = None
            for key, cat_id in category_mapping.items():
                if key in section_title:
                    current_category = cat_id
                    current_section = section_title
                    break

            continue

        # æå–èµ„æºé“¾æ¥
        # Extract resource links
        if current_category and line.strip().startswith("-"):
            resource_data = extract_link_and_text(line)

            if resource_data:
                category_info = categories.get(current_category, {})
                prefix = category_info.get("prefix", "res")

                # ç”Ÿæˆå”¯ä¸€ ID
                # Generate unique ID
                resource_id = generate_resource_id(resource_data["name"], prefix)

                # æ£€æŸ¥æ˜¯å¦ä¸ºå®˜æ–¹èµ„æºï¼ˆåœ¨å®˜æ–¹èµ„æºåˆ†ç±»ä¸­ï¼‰
                # Check if official resource (in official-resources category)
                is_official = current_category == "official-resources" or resource_data["is_pinned"]

                resource = {
                    "ID": resource_id,
                    "DisplayName": resource_data["name"],
                    "DisplayName_ZH": resource_data["name"],  # å°†è¢«æ‰‹åŠ¨æ›´æ–° / To be manually updated
                    "Category": current_category,
                    "SubCategory": "general",
                    "PrimaryLink": resource_data["url"],
                    "SecondaryLink": "",
                    "Author": "",  # éœ€è¦æ‰‹åŠ¨è¡¥å…… / To be manually added
                    "AuthorProfile": "",
                    "IsActive": "TRUE",
                    "DateAdded": datetime.now().strftime("%Y-%m-%d"),
                    "LastModified": datetime.now().strftime("%Y-%m-%d"),
                    "LastChecked": "",
                    "License": "",  # éœ€è¦æ‰‹åŠ¨è¡¥å…… / To be manually added
                    "Description": resource_data["description"],
                    "Description_ZH": resource_data["description"],
                    "Tags_ZH": current_section if current_section else "",
                    "IsPinned": "TRUE" if is_official else "FALSE",
                    "Section": "official" if is_official else "community",
                }

                resources.append(resource)

    return resources


def write_csv(resources: List[Dict], output_file: Path):
    """
    å†™å…¥ CSV æ–‡ä»¶
    Write CSV file
    """
    if not resources:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æå–åˆ°èµ„æº / Warning: No resources extracted")
        return

    fieldnames = [
        "ID",
        "DisplayName",
        "DisplayName_ZH",
        "Category",
        "SubCategory",
        "PrimaryLink",
        "SecondaryLink",
        "Author",
        "AuthorProfile",
        "IsActive",
        "DateAdded",
        "LastModified",
        "LastChecked",
        "License",
        "Description",
        "Description_ZH",
        "Tags_ZH",
        "IsPinned",
        "Section",
    ]

    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(resources)

    print(f"âœ… æˆåŠŸå†™å…¥ {len(resources)} æ¡èµ„æºåˆ° {output_file}")
    print(f"âœ… Successfully wrote {len(resources)} resources to {output_file}")


def generate_migration_report(resources: List[Dict], output_file: Path):
    """
    ç”Ÿæˆè¿ç§»æŠ¥å‘Š
    Generate migration report
    """
    report = []
    report.append("# èµ„æºè¿ç§»æŠ¥å‘Š / Resource Migration Report")
    report.append(f"\nç”Ÿæˆæ—¶é—´ / Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"\næ€»èµ„æºæ•° / Total Resources: {len(resources)}")

    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    # Statistics by category
    category_counts = {}
    official_count = 0

    for res in resources:
        cat = res["Category"]
        category_counts[cat] = category_counts.get(cat, 0) + 1
        if res["IsPinned"] == "TRUE":
            official_count += 1

    report.append(f"\nå®˜æ–¹èµ„æº / Official Resources: {official_count}")
    report.append(f"ç¤¾åŒºèµ„æº / Community Resources: {len(resources) - official_count}")

    report.append("\n## åˆ†ç±»ç»Ÿè®¡ / Category Statistics\n")
    for cat, count in sorted(category_counts.items()):
        report.append(f"- {cat}: {count}")

    report.append("\n## éœ€è¦æ‰‹åŠ¨è¡¥å……çš„å­—æ®µ / Fields Requiring Manual Input\n")
    report.append("ä»¥ä¸‹å­—æ®µéœ€è¦æ‰‹åŠ¨è¡¥å……ï¼š")
    report.append("The following fields require manual input:")
    report.append("1. **Author** - èµ„æºä½œè€…åç§° / Resource author name")
    report.append("2. **AuthorProfile** - ä½œè€…ä¸»é¡µé“¾æ¥ / Author profile link")
    report.append("3. **License** - è®¸å¯è¯ç±»å‹ï¼ˆå¦‚ MIT, Apache-2.0ï¼‰/ License type (e.g., MIT, Apache-2.0)")
    report.append("4. **DisplayName_ZH** - å®¡æ ¸å¹¶ä¼˜åŒ–ä¸­æ–‡æ˜¾ç¤ºå / Review and optimize Chinese display name")
    report.append("5. **Description_ZH** - å®¡æ ¸å¹¶ä¼˜åŒ–ä¸­æ–‡æè¿° / Review and optimize Chinese description")

    report.append("\n## åç»­æ­¥éª¤ / Next Steps\n")
    report.append("1. å®¡æŸ¥ç”Ÿæˆçš„ CSV æ–‡ä»¶ / Review generated CSV file")
    report.append("2. æ‰‹åŠ¨è¡¥å……ä½œè€…å’Œè®¸å¯è¯ä¿¡æ¯ / Manually add author and license info")
    report.append("3. ä¼˜åŒ–ä¸­æ–‡å­—æ®µç¿»è¯‘ / Optimize Chinese field translations")
    report.append("4. è¿è¡ŒéªŒè¯è„šæœ¬æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ / Run validation script to check data integrity")
    report.append("5. ç”Ÿæˆ README.md / Generate README.md")

    report_content = "\n".join(report)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report_content)

    print(f"\nğŸ“„ è¿ç§»æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    print(f"ğŸ“„ Migration report generated: {output_file}")

    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    # Also print to console
    print("\n" + "=" * 60)
    print(report_content)
    print("=" * 60)


def main():
    """ä¸»å‡½æ•° / Main function"""
    # è®¾ç½®è·¯å¾„
    # Setup paths
    project_root = Path(__file__).parent.parent
    readme_file = project_root / "README.md"
    categories_file = project_root / "templates" / "categories.yaml"
    output_csv = project_root / "THE_RESOURCES_TABLE.csv"
    report_file = project_root / "MIGRATION_REPORT.md"

    print("ğŸš€ å¼€å§‹è¿ç§»èµ„æº / Starting resource migration...")
    print(f"ğŸ“– è¯»å– README: {readme_file}")
    print(f"ğŸ“‹ è¯»å–åˆ†ç±»å®šä¹‰: {categories_file}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    # Check if files exist
    if not readme_file.exists():
        print(f"âŒ é”™è¯¯ï¼šREADME.md ä¸å­˜åœ¨: {readme_file}")
        return

    if not categories_file.exists():
        print(f"âŒ é”™è¯¯ï¼šcategories.yaml ä¸å­˜åœ¨: {categories_file}")
        return

    # åŠ è½½åˆ†ç±»
    # Load categories
    categories = load_categories(categories_file)
    print(f"âœ… åŠ è½½äº† {len(categories)} ä¸ªåˆ†ç±»")

    # è§£æ README
    # Parse README
    resources = parse_readme(readme_file, categories)
    print(f"âœ… æå–äº† {len(resources)} ä¸ªèµ„æº")

    # å†™å…¥ CSV
    # Write CSV
    write_csv(resources, output_csv)

    # ç”ŸæˆæŠ¥å‘Š
    # Generate report
    generate_migration_report(resources, report_file)

    print("\nâœ¨ è¿ç§»å®Œæˆï¼/ Migration complete!")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print("Next steps:")
    print(f"1. æ£€æŸ¥ {output_csv}")
    print(f"2. é˜…è¯» {report_file}")
    print("3. æ‰‹åŠ¨è¡¥å……å…ƒæ•°æ®")
    print("4. è¿è¡Œ 'make generate' ç”Ÿæˆ README")


if __name__ == "__main__":
    main()
