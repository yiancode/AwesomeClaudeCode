#!/usr/bin/env python3
"""
å¤šæºèµ„æºçˆ¬å–è„šæœ¬ / Multi-source Resource Crawl Script

ç»Ÿä¸€è¿è¡Œæ‰€æœ‰çˆ¬è™«ï¼Œä»å¤šä¸ªæ¥æºå‘ç°ä¸ Claude Code ç›¸å…³çš„èµ„æºã€‚
Runs all crawlers to discover Claude Code related resources from multiple sources.

ç”¨æ³• / Usage:
    python scripts/multi_source_crawl.py [--dry-run] [--sources SOURCE1,SOURCE2] [--limit N]
"""

import argparse
import sys
from pathlib import Path
from typing import List, Optional

import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.crawlers import (
    RedditCrawler,
    AwesomeListCrawler,
    RSSCrawler,
    HackerNewsCrawler,
)


def load_config() -> dict:
    """åŠ è½½çˆ¬è™«é…ç½® / Load crawler configuration"""
    config_file = PROJECT_ROOT / "config" / "crawlers.yaml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def get_available_crawlers() -> dict:
    """è·å–å¯ç”¨çš„çˆ¬è™« / Get available crawlers"""
    return {
        'reddit': RedditCrawler,
        'awesome': AwesomeListCrawler,
        'rss': RSSCrawler,
        'hackernews': HackerNewsCrawler,
    }


def run_crawler(
    crawler_class,
    config: dict,
    dry_run: bool = False,
    limit: int = 10
) -> tuple:
    """
    è¿è¡Œå•ä¸ªçˆ¬è™« / Run single crawler

    Args:
        crawler_class: çˆ¬è™«ç±» / Crawler class
        config: é…ç½® / Configuration
        dry_run: æ˜¯å¦ä¸ºæ¼”ç¤ºæ¨¡å¼ / Whether in dry run mode
        limit: æœ€å¤§èµ„æºæ•°é‡ / Maximum number of resources

    Returns:
        (å‘ç°æ•°é‡, æ·»åŠ æ•°é‡) / (discovered count, added count)
    """
    rate_limits = config.get('rate_limits', {})
    crawler = crawler_class(config, rate_limits)

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨
    source_config = config.get(crawler.source_type, {})
    if not source_config.get('enabled', True):
        print(f"   â­ï¸ {crawler.name} å·²ç¦ç”¨ï¼Œè·³è¿‡")
        return 0, 0

    return crawler.run(dry_run=dry_run, limit=limit)


def main():
    """ä¸»å‡½æ•° / Main function"""
    parser = argparse.ArgumentParser(description='Multi-source resource crawl')
    parser.add_argument('--dry-run', action='store_true', help='Do not modify files')
    parser.add_argument('--sources', type=str, default='all',
                        help='Comma-separated list of sources (reddit,awesome,rss,hackernews) or "all"')
    parser.add_argument('--limit', type=int, default=10, help='Maximum resources per source')
    args = parser.parse_args()

    print("ğŸ•¸ï¸  å¤šæºèµ„æºçˆ¬å– / Multi-source Resource Crawl")
    print("=" * 50)

    # åŠ è½½é…ç½®
    print("\nğŸ“‚ åŠ è½½é…ç½®...")
    config = load_config()

    # è·å–è¦è¿è¡Œçš„çˆ¬è™«
    available_crawlers = get_available_crawlers()

    if args.sources == 'all':
        sources_to_run = list(available_crawlers.keys())
    else:
        sources_to_run = [s.strip().lower() for s in args.sources.split(',')]
        # éªŒè¯æ¥æº
        for source in sources_to_run:
            if source not in available_crawlers:
                print(f"âŒ æœªçŸ¥çš„æ•°æ®æº: {source}")
                print(f"   å¯ç”¨: {', '.join(available_crawlers.keys())}")
                return 1

    print(f"   å°†è¿è¡Œ: {', '.join(sources_to_run)}")

    if args.dry_run:
        print("   [Dry Run] æ¨¡å¼ï¼šä¸ä¼šä¿å­˜ä»»ä½•æ•°æ®")

    # è¿è¡Œçˆ¬è™«
    total_discovered = 0
    total_added = 0
    results = {}

    for source in sources_to_run:
        crawler_class = available_crawlers[source]

        try:
            discovered, added = run_crawler(
                crawler_class,
                config,
                dry_run=args.dry_run,
                limit=args.limit
            )

            results[source] = {'discovered': discovered, 'added': added}
            total_discovered += discovered
            total_added += added

        except Exception as e:
            print(f"   âŒ {source} çˆ¬å–å¤±è´¥: {e}")
            results[source] = {'discovered': 0, 'added': 0, 'error': str(e)}

    # è¾“å‡ºæ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ“Š çˆ¬å–æ‘˜è¦ / Crawl Summary")
    print("=" * 50)

    print(f"\n{'æ¥æº':<15} {'å‘ç°':<10} {'æ·»åŠ ':<10}")
    print("-" * 35)
    for source, result in results.items():
        discovered = result.get('discovered', 0)
        added = result.get('added', 0)
        error = result.get('error')

        if error:
            print(f"{source:<15} {'é”™è¯¯':<10} {error}")
        else:
            print(f"{source:<15} {discovered:<10} {added:<10}")

    print("-" * 35)
    print(f"{'æ€»è®¡':<15} {total_discovered:<10} {total_added:<10}")

    print("\nâœ… å®Œæˆï¼")

    # è¾“å‡ºä¾› GitHub Actions ä½¿ç”¨
    print(f"::set-output name=total_discovered::{total_discovered}")
    print(f"::set-output name=total_added::{total_added}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
