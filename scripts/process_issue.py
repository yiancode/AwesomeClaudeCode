#!/usr/bin/env python3
"""
Issue è‡ªåŠ¨å¤„ç†è„šæœ¬ / Issue Automatic Processing Script

ä» GitHub Issue è¡¨å•è§£æèµ„æºæäº¤ï¼Œå¹¶æ·»åŠ åˆ°å€™é€‰é˜Ÿåˆ—ã€‚
Parses resource submissions from GitHub Issue forms and adds them to the candidate queue.

ç”¨æ³• / Usage:
    python scripts/process_issue.py --issue-number 123 --issue-body "..."
    æˆ– / or:
    è®¾ç½®ç¯å¢ƒå˜é‡ ISSUE_NUMBER å’Œ ISSUE_BODY
"""

import argparse
import hashlib
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse

import requests
import yaml


# é¡¹ç›®æ ¹ç›®å½• / Project root
PROJECT_ROOT = Path(__file__).parent.parent

# åˆ†ç±»åç§°æ˜ å°„ï¼ˆä» Issue è¡¨å•é€‰é¡¹åˆ° category IDï¼‰
# Category name mapping (from Issue form options to category IDs)
CATEGORY_MAPPING = {
    "ğŸ›ï¸ å®˜æ–¹èµ„æº / Official Documentation": "official-resources",
    "ğŸ¤– ä»£ç†æŠ€èƒ½ / Agent Skills": "skills",
    "ğŸ§  å·¥ä½œæµä¸çŸ¥è¯†æŒ‡å— / Workflows & Knowledge Guides": "workflows",
    "ğŸ§° å·¥å…· / Tooling": "tooling",
    "ğŸ“Š çŠ¶æ€æ  / Status Lines": "statusline",
    "ğŸª é’©å­ / Hooks": "hooks",
    "ğŸ”ª æ–œæ å‘½ä»¤ / Slash-Commands": "slash-commands",
    "ğŸ“‚ CLAUDE.md æ–‡ä»¶ / CLAUDE.md Files": "claude-md-files",
    "ğŸ“± æ›¿ä»£å®¢æˆ·ç«¯ / Alternative Clients": "alternative-clients",
    "ğŸ”Œ MCP æœåŠ¡å™¨ / MCP Servers": "mcp-servers",
    "ğŸ“¦ å¼€æºé¡¹ç›® / Open Source Projects": "open-source-projects",
    "ğŸ“‚ æ¡ˆä¾‹ç ”ç©¶ / Case Studies": "case-studies",
    "ğŸŒ ç”Ÿæ€ç³»ç»Ÿ / Ecosystem": "ecosystem",
}

# å­åˆ†ç±»æ˜ å°„ / Subcategory mapping
SUBCATEGORY_MAPPING = {
    "é€šç”¨ / General": "general",
    "API ä¸æ–‡æ¡£ / API & Documentation": "api-docs",
    "SDK åº“ / SDK Libraries": "sdk-libraries",
    "æ•™ç¨‹ä¸ç¤ºä¾‹ / Tutorials & Examples": "tutorials",
    "æœ€ä½³å®è·µ / Best Practices": "best-practices",
    "IDE é›†æˆ / IDE Integrations": "ide-integrations",
    "ä½¿ç”¨ç›‘æ§ / Usage Monitors": "usage-monitors",
    "ç¼–æ’å™¨ / Orchestrators": "orchestrators",
    "ç‰ˆæœ¬æ§åˆ¶ä¸ Git / Version Control & Git": "version-control-git",
    "ä»£ç åˆ†æä¸æµ‹è¯• / Code Analysis & Testing": "code-analysis-testing",
    "ä¸Šä¸‹æ–‡åŠ è½½ä¸é¢„çƒ­ / Context Loading & Priming": "context-loading-priming",
    "æ–‡æ¡£ä¸å˜æ›´æ—¥å¿— / Documentation & Changelogs": "documentation-changelogs",
    "æŒç»­é›†æˆ / éƒ¨ç½² / CI / Deployment": "ci-deployment",
    "é¡¹ç›®ä¸ä»»åŠ¡ç®¡ç† / Project & Task Management": "project-task-management",
    "ç‰¹å®šè¯­è¨€ / Language-Specific": "language-specific",
    "ç‰¹å®šé¢†åŸŸ / Domain-Specific": "domain-specific",
    "é¡¹ç›®è„šæ‰‹æ¶ä¸ MCP / Project Scaffolding & MCP": "project-scaffolding-mcp",
    "æ–‡ä»¶ç³»ç»Ÿ / Filesystem": "filesystem",
    "äº‘æœåŠ¡ / Cloud Services": "cloud-services",
    "æ•°æ®åº“ / Databases": "databases",
    "API é›†æˆ / API Integrations": "api-integrations",
    "æ¨¡æ¿ / Templates": "templates",
    "æ‰©å±• / Extensions": "extensions",
    "Web å¼€å‘ / Web Development": "web-development",
    "ç§»åŠ¨å¼€å‘ / Mobile Development": "mobile-development",
    "æ•°æ®ç§‘å­¦ / Data Science": "data-science",
    "DevOps": "devops",
    "å­¦ä¹ èµ„æº / Learning Resources": "learning-resources",
    "ç¤¾åŒº / Community": "community",
    "ç¬¬ä¸‰æ–¹å·¥å…· / Third-party Tools": "third-party-tools",
    "å…¶ä»– / Miscellaneous": "miscellaneous",
}


def load_categories() -> dict:
    """
    åŠ è½½åˆ†ç±»å®šä¹‰ä»¥è·å– prefix æ˜ å°„
    Load category definitions to get prefix mapping
    """
    categories_file = PROJECT_ROOT / "templates" / "categories.yaml"
    with open(categories_file, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)

    # åˆ›å»º category_id -> prefix æ˜ å°„
    return {cat['id']: cat['prefix'] for cat in data['categories']}


def generate_resource_id(category_id: str, url: str, categories_prefix: dict) -> str:
    """
    ç”Ÿæˆèµ„æº ID / Generate resource ID
    æ ¼å¼: {prefix}-{hash8}
    """
    prefix = categories_prefix.get(category_id, 'res')
    # ä½¿ç”¨ URL ç”Ÿæˆ hash
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    return f"{prefix}-{url_hash}"


def parse_issue_body(body: str) -> dict:
    """
    è§£æ Issue è¡¨å•å†…å®¹ / Parse Issue form content

    GitHub Issue è¡¨å•ä½¿ç”¨ç‰¹å®šæ ¼å¼:
    ### æ ‡ç­¾å
    å€¼

    ### å¦ä¸€ä¸ªæ ‡ç­¾
    å¦ä¸€ä¸ªå€¼
    """
    result = {}

    # ä½¿ç”¨æ­£åˆ™åŒ¹é… ### æ ‡é¢˜ å’Œå…¶åçš„å†…å®¹
    # Match ### headers and their content
    pattern = r'###\s*(.+?)\s*\n(.*?)(?=###|\Z)'
    matches = re.findall(pattern, body, re.DOTALL)

    for label, value in matches:
        # æ¸…ç†æ ‡ç­¾å’Œå€¼
        label = label.strip()
        value = value.strip()

        # ç§»é™¤ "æ— å“åº”" / "_No response_" å ä½ç¬¦
        if value.lower() in ['_no response_', 'æ— å“åº”', '']:
            value = ''

        result[label] = value

    return result


def extract_descriptions(description_text: str) -> tuple:
    """
    ä»æè¿°æ–‡æœ¬ä¸­åˆ†ç¦»ä¸­è‹±æ–‡æè¿°
    Separate Chinese and English descriptions from description text

    Returns: (description_zh, description_en)
    """
    if not description_text:
        return '', ''

    lines = description_text.strip().split('\n')
    desc_zh = ''
    desc_en = ''

    current_lang = None
    current_text = []

    for line in lines:
        line_lower = line.lower().strip()

        # æ£€æµ‹è¯­è¨€æ ‡è®°
        if 'ä¸­æ–‡æè¿°' in line or 'chinese description' in line_lower:
            if current_lang == 'en' and current_text:
                desc_en = ' '.join(current_text).strip()
            current_lang = 'zh'
            current_text = []
        elif 'è‹±æ–‡æè¿°' in line or 'english description' in line_lower:
            if current_lang == 'zh' and current_text:
                desc_zh = ' '.join(current_text).strip()
            current_lang = 'en'
            current_text = []
        elif line.strip():
            current_text.append(line.strip())

    # å¤„ç†æœ€åä¸€æ®µ
    if current_lang == 'zh' and current_text:
        desc_zh = ' '.join(current_text).strip()
    elif current_lang == 'en' and current_text:
        desc_en = ' '.join(current_text).strip()
    elif current_text and not desc_zh:
        # å¦‚æœæ²¡æœ‰æ˜ç¡®æ ‡è®°ï¼Œå‡è®¾æ˜¯ä¸­æ–‡
        desc_zh = ' '.join(current_text).strip()

    return desc_zh, desc_en


def validate_url(url: str, timeout: int = 10) -> tuple:
    """
    éªŒè¯ URL æ˜¯å¦å¯è®¿é—®
    Validate if URL is accessible

    Returns: (is_valid, status_code, error_message)
    """
    if not url:
        return False, 0, "URL ä¸ºç©º / URL is empty"

    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False, 0, "URL æ ¼å¼æ— æ•ˆ / Invalid URL format"

        headers = {
            'User-Agent': 'AwesomeClaudeCode-Bot/1.0 (+https://github.com/yiancode/AwesomeClaudeCode)'
        }

        response = requests.head(url, timeout=timeout, headers=headers, allow_redirects=True)

        if response.status_code < 400:
            return True, response.status_code, None
        else:
            return False, response.status_code, f"HTTP {response.status_code}"

    except requests.exceptions.Timeout:
        return False, 0, "è¯·æ±‚è¶…æ—¶ / Request timeout"
    except requests.exceptions.ConnectionError:
        return False, 0, "è¿æ¥å¤±è´¥ / Connection failed"
    except Exception as e:
        return False, 0, str(e)


def check_duplicate(url: str, pending_file: Path, rejected_file: Path) -> tuple:
    """
    æ£€æŸ¥ URL æ˜¯å¦å·²å­˜åœ¨ï¼ˆåœ¨å¾…å®¡æ ¸æˆ–å·²æ‹’ç»åˆ—è¡¨ä¸­ï¼‰
    Check if URL already exists (in pending or rejected list)

    Returns: (is_duplicate, location)
    """
    # è§„èŒƒåŒ– URL
    normalized_url = url.rstrip('/').lower()

    # æ£€æŸ¥å¾…å®¡æ ¸åˆ—è¡¨
    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for res in data.get('resources', []):
                if res.get('PrimaryLink', '').rstrip('/').lower() == normalized_url:
                    return True, 'pending'

    # æ£€æŸ¥å·²æ‹’ç»åˆ—è¡¨
    if rejected_file.exists():
        with open(rejected_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for res in data.get('resources', []):
                if res.get('PrimaryLink', '').rstrip('/').lower() == normalized_url:
                    return True, 'rejected'

    # æ£€æŸ¥ä¸» CSV
    csv_file = PROJECT_ROOT / 'THE_RESOURCES_TABLE.csv'
    if csv_file.exists():
        import csv
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('PrimaryLink', '').rstrip('/').lower() == normalized_url:
                    return True, 'csv'

    return False, None


def extract_github_info(url: str) -> dict:
    """
    ä» GitHub URL æå–ä½œè€…ä¿¡æ¯
    Extract author info from GitHub URL
    """
    result = {'author': '', 'author_profile': ''}

    parsed = urlparse(url)
    if 'github.com' not in parsed.netloc:
        return result

    # æå– owner/repo
    path_parts = [p for p in parsed.path.split('/') if p]
    if len(path_parts) >= 1:
        owner = path_parts[0]
        result['author'] = owner
        result['author_profile'] = f"https://github.com/{owner}"

    return result


def create_candidate_resource(parsed_data: dict, issue_number: int, categories_prefix: dict) -> dict:
    """
    ä»è§£æçš„ Issue æ•°æ®åˆ›å»ºå€™é€‰èµ„æº
    Create candidate resource from parsed Issue data
    """
    # æå–å­—æ®µï¼ˆä½¿ç”¨å¤šç§å¯èƒ½çš„æ ‡ç­¾åï¼‰
    name = (parsed_data.get('èµ„æºåç§° / Resource Name') or
            parsed_data.get('èµ„æºåç§°') or
            parsed_data.get('Resource Name', '')).strip()

    url = (parsed_data.get('èµ„æºé“¾æ¥ / Resource URL') or
           parsed_data.get('èµ„æºé“¾æ¥') or
           parsed_data.get('Resource URL', '')).strip()

    category_raw = (parsed_data.get('ä¸»åˆ†ç±» / Primary Category') or
                    parsed_data.get('ä¸»åˆ†ç±»') or
                    parsed_data.get('Primary Category', '')).strip()

    subcategory_raw = (parsed_data.get('å­åˆ†ç±» / Subcategory (å¯é€‰ / Optional)') or
                       parsed_data.get('å­åˆ†ç±»') or
                       parsed_data.get('Subcategory', '')).strip()

    description_raw = (parsed_data.get('èµ„æºæè¿° / Resource Description') or
                       parsed_data.get('èµ„æºæè¿°') or
                       parsed_data.get('Resource Description', '')).strip()

    author = (parsed_data.get('ä½œè€… / Author (å¯é€‰ / Optional)') or
              parsed_data.get('ä½œè€…') or
              parsed_data.get('Author', '')).strip()

    author_profile = (parsed_data.get('ä½œè€…ä¸»é¡µ / Author Profile (å¯é€‰ / Optional)') or
                      parsed_data.get('ä½œè€…ä¸»é¡µ') or
                      parsed_data.get('Author Profile', '')).strip()

    license_info = (parsed_data.get('è®¸å¯è¯ / License (å¯é€‰ / Optional)') or
                    parsed_data.get('è®¸å¯è¯') or
                    parsed_data.get('License', '')).strip()

    secondary_link = (parsed_data.get('å¤‡ç”¨é“¾æ¥ / Secondary Link (å¯é€‰ / Optional)') or
                      parsed_data.get('å¤‡ç”¨é“¾æ¥') or
                      parsed_data.get('Secondary Link', '')).strip()

    # æ˜ å°„åˆ†ç±»
    category_id = CATEGORY_MAPPING.get(category_raw, 'ecosystem')
    subcategory_id = SUBCATEGORY_MAPPING.get(subcategory_raw, 'general')

    # åˆ†ç¦»ä¸­è‹±æ–‡æè¿°
    desc_zh, desc_en = extract_descriptions(description_raw)
    if not desc_en:
        desc_en = desc_zh  # å¦‚æœæ²¡æœ‰è‹±æ–‡ï¼Œä½¿ç”¨ä¸­æ–‡

    # å¦‚æœæ²¡æœ‰æä¾›ä½œè€…ä¿¡æ¯ï¼Œå°è¯•ä» GitHub URL æå–
    if not author and 'github.com' in url:
        github_info = extract_github_info(url)
        author = github_info['author']
        if not author_profile:
            author_profile = github_info['author_profile']

    # ç”Ÿæˆèµ„æº ID
    resource_id = generate_resource_id(category_id, url, categories_prefix)

    # å½“å‰æ—¥æœŸ
    today = datetime.now().strftime('%Y/%m/%d')

    # æ„å»ºèµ„æºå¯¹è±¡
    resource = {
        'ID': resource_id,
        'DisplayName': name,
        'DisplayName_ZH': name,  # Issue æäº¤é€šå¸¸æ˜¯ä¸­æ–‡å
        'Category': category_id,
        'SubCategory': subcategory_id,
        'PrimaryLink': url,
        'SecondaryLink': secondary_link,
        'Author': author,
        'AuthorProfile': author_profile,
        'IsActive': 'TRUE',
        'DateAdded': today,
        'LastModified': today,
        'LastChecked': today,
        'License': license_info,
        'Description': desc_en,
        'Description_ZH': desc_zh,
        'Tags_ZH': '',
        'IsPinned': 'FALSE',
        'Section': 'community',
        # å…ƒæ•°æ®ï¼ˆä¸ä¼šå†™å…¥ CSVï¼‰
        '_source_issue': issue_number,
        '_submitted_at': datetime.now().isoformat(),
        '_status': 'pending',
    }

    return resource


def add_to_pending(resource: dict, pending_file: Path) -> bool:
    """
    æ·»åŠ èµ„æºåˆ°å¾…å®¡æ ¸é˜Ÿåˆ—
    Add resource to pending queue
    """
    # åŠ è½½ç°æœ‰æ•°æ®
    if pending_file.exists():
        with open(pending_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {
            "_comment": "å€™é€‰èµ„æºé˜Ÿåˆ— - å¾…å®¡æ ¸çš„èµ„æº / Candidate resource queue - resources pending review",
            "_schema_version": "1.0",
            "resources": []
        }

    # æ·»åŠ æ–°èµ„æº
    data['resources'].append(resource)

    # ä¿å­˜
    with open(pending_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return True


def main():
    """ä¸»å‡½æ•° / Main function"""
    parser = argparse.ArgumentParser(description='Process GitHub Issue for resource submission')
    parser.add_argument('--issue-number', type=int, help='Issue number')
    parser.add_argument('--issue-body', type=str, help='Issue body content')
    parser.add_argument('--dry-run', action='store_true', help='Do not modify files')
    args = parser.parse_args()

    # ä»å‚æ•°æˆ–ç¯å¢ƒå˜é‡è·å–
    issue_number = args.issue_number or int(os.environ.get('ISSUE_NUMBER', 0))
    issue_body = args.issue_body or os.environ.get('ISSUE_BODY', '')

    if not issue_number or not issue_body:
        print("âŒ é”™è¯¯ï¼šéœ€è¦æä¾› issue-number å’Œ issue-body")
        print("âŒ Error: issue-number and issue-body are required")
        sys.exit(1)

    print(f"ğŸ“‹ å¤„ç† Issue #{issue_number}...")
    print(f"ğŸ“‹ Processing Issue #{issue_number}...")

    # åŠ è½½åˆ†ç±»é…ç½®
    categories_prefix = load_categories()

    # è§£æ Issue å†…å®¹
    print("\nğŸ” è§£æ Issue å†…å®¹...")
    parsed = parse_issue_body(issue_body)

    if not parsed:
        print("âŒ æ— æ³•è§£æ Issue å†…å®¹")
        print("âŒ Failed to parse Issue content")
        sys.exit(1)

    print(f"   æ‰¾åˆ° {len(parsed)} ä¸ªå­—æ®µ")

    # æå– URL è¿›è¡ŒéªŒè¯
    url = (parsed.get('èµ„æºé“¾æ¥ / Resource URL') or
           parsed.get('èµ„æºé“¾æ¥') or
           parsed.get('Resource URL', '')).strip()

    if not url:
        print("âŒ æœªæ‰¾åˆ°èµ„æºé“¾æ¥")
        print("âŒ Resource URL not found")
        sys.exit(1)

    print(f"\nğŸ”— éªŒè¯ URL: {url}")

    # æ£€æŸ¥é‡å¤
    pending_file = PROJECT_ROOT / 'candidates' / 'pending_resources.json'
    rejected_file = PROJECT_ROOT / 'candidates' / 'rejected_resources.json'

    is_dup, dup_location = check_duplicate(url, pending_file, rejected_file)
    if is_dup:
        print(f"âš ï¸  å‘ç°é‡å¤èµ„æº (åœ¨ {dup_location} ä¸­)")
        print(f"âš ï¸  Duplicate resource found (in {dup_location})")
        # è¾“å‡ºç»“æœä¾› GitHub Actions ä½¿ç”¨
        print("::set-output name=status::duplicate")
        print(f"::set-output name=duplicate_location::{dup_location}")
        sys.exit(0)  # ä¸æ˜¯é”™è¯¯ï¼Œåªæ˜¯é‡å¤

    # éªŒè¯ URL å¯è®¿é—®æ€§
    is_valid, status_code, error = validate_url(url)
    if not is_valid:
        print(f"âš ï¸  URL éªŒè¯å¤±è´¥: {error}")
        print(f"âš ï¸  URL validation failed: {error}")
        # ä»ç„¶å¯ä»¥ç»§ç»­ï¼Œä½†æ ‡è®°çŠ¶æ€
        print("::set-output name=url_status::invalid")
        print(f"::set-output name=url_error::{error}")
    else:
        print(f"   âœ… URL æœ‰æ•ˆ (HTTP {status_code})")

    # åˆ›å»ºå€™é€‰èµ„æº
    print("\nğŸ“¦ åˆ›å»ºå€™é€‰èµ„æº...")
    resource = create_candidate_resource(parsed, issue_number, categories_prefix)

    print(f"   ID: {resource['ID']}")
    print(f"   åç§°: {resource['DisplayName']}")
    print(f"   åˆ†ç±»: {resource['Category']}/{resource['SubCategory']}")
    print(f"   ä½œè€…: {resource['Author']}")

    if args.dry_run:
        print("\nğŸ” [Dry Run] èµ„æºå†…å®¹:")
        print(json.dumps(resource, ensure_ascii=False, indent=2))
        sys.exit(0)

    # æ·»åŠ åˆ°å¾…å®¡æ ¸é˜Ÿåˆ—
    print("\nğŸ’¾ æ·»åŠ åˆ°å¾…å®¡æ ¸é˜Ÿåˆ—...")
    add_to_pending(resource, pending_file)

    print("\nâœ… å¤„ç†å®Œæˆï¼")
    print("âœ… Processing complete!")

    # è¾“å‡ºç»“æœä¾› GitHub Actions ä½¿ç”¨
    print("::set-output name=status::success")
    print(f"::set-output name=resource_id::{resource['ID']}")
    print(f"::set-output name=resource_name::{resource['DisplayName']}")
    print(f"::set-output name=category::{resource['Category']}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
